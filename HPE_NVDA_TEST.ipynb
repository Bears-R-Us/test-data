{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c271641c-fb06-4735-a64d-de5a4670d1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, BooleanType, DateType, TimestampType, ArrayType\n",
    ")\n",
    "# Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ExcelMetadataWorkflow\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb5f91a-4074-458b-8227-b2db8a003e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\r\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\n\u001B[?25l\r\u001B[K     |█▎                              | 10 kB 8.3 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 20 kB 13.9 MB/s eta 0:00:01\r\u001B[K     |████                            | 30 kB 14.8 MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 40 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |██████▌                         | 51 kB 6.0 MB/s eta 0:00:01\r\u001B[K     |███████▉                        | 61 kB 7.0 MB/s eta 0:00:01\r\u001B[K     |█████████▏                      | 71 kB 7.7 MB/s eta 0:00:01\r\u001B[K     |██████████▌                     | 81 kB 7.8 MB/s eta 0:00:01\r\u001B[K     |███████████▊                    | 92 kB 8.7 MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 102 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██████████████▍                 | 112 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████▊                | 122 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 133 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████▎             | 143 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████▋            | 153 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 163 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 174 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▌        | 184 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▉       | 194 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▏     | 204 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▍    | 215 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▊   | 225 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████  | 235 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▍| 245 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 250 kB 6.3 MB/s \r\n\u001B[?25hCollecting et-xmlfile\r\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\r\nInstalling collected packages: et-xmlfile, openpyxl\r\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-4837e831-749c-4365-82ae-f7091d969838/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171fc0c9-c0b0-4609-875e-edd205561ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158464>:7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m excel_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/HPE_NVDA_datagen.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# update this path\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m sheets \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(excel_path, sheet_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# sheets = spark.read.  \u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m sheet_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(sheets\u001B[38;5;241m.\u001B[39mkeys())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n",
       "\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n",
       "\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n",
       "\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n",
       "\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n",
       "\u001B[1;32m    310\u001B[0m     )\n",
       "\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:457\u001B[0m, in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n",
       "\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n",
       "\u001B[1;32m    456\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m--> 457\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n",
       "\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    460\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    462\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:1376\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n",
       "\u001B[1;32m   1374\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1376\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n",
       "\u001B[1;32m   1378\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1379\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1380\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1381\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1382\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1383\u001B[0m         )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:1250\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[0;34m(content_or_path, storage_options)\u001B[0m\n",
       "\u001B[1;32m   1247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n",
       "\u001B[1;32m   1248\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n",
       "\u001B[0;32m-> 1250\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n",
       "\u001B[1;32m   1252\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n",
       "\u001B[1;32m   1253\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n",
       "\u001B[1;32m   1254\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/common.py:798\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n",
       "\u001B[1;32m    789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n",
       "\u001B[1;32m    790\u001B[0m             handle,\n",
       "\u001B[1;32m    791\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    794\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    795\u001B[0m         )\n",
       "\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n",
       "\u001B[0;32m--> 798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    799\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n",
       "\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/HPE_NVDA_datagen.xlsx'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158464>:7\u001B[0m\n\u001B[1;32m      4\u001B[0m excel_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/HPE_NVDA_datagen.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# update this path\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m sheets \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(excel_path, sheet_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# sheets = spark.read.  \u001B[39;00m\n\u001B[1;32m      9\u001B[0m sheet_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(sheets\u001B[38;5;241m.\u001B[39mkeys())\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:457\u001B[0m, in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[1;32m    456\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 457\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    460\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    462\u001B[0m     )\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:1376\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[1;32m   1374\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1376\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1379\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1380\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1381\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1382\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1383\u001B[0m         )\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/excel/_base.py:1250\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[0;34m(content_or_path, storage_options)\u001B[0m\n\u001B[1;32m   1247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[1;32m   1248\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[0;32m-> 1250\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m   1252\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[1;32m   1253\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[1;32m   1254\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/io/common.py:798\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    790\u001B[0m             handle,\n\u001B[1;32m    791\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    794\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    795\u001B[0m         )\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n\n\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/HPE_NVDA_datagen.xlsx'",
       "errorSummary": "<span class='ansi-red-fg'>FileNotFoundError</span>: [Errno 2] No such file or directory: '/tmp/HPE_NVDA_datagen.xlsx'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 1. Read all sheets from the Excel file.\n",
    "# -------------------------------------\n",
    "excel_path = \"/tmp/HPE_NVDA_datagen.xlsx\"  # update this path\n",
    "\n",
    "# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "# sheets = spark.read.  \n",
    "sheet_names = list(sheets.keys())\n",
    "print(\"Found sheets:\", sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834dbb3a-fefd-4d82-941b-0f3afb06dc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158465>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 2. Process the tables overview (first sheet)\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m tables_overview_df \u001B[38;5;241m=\u001B[39m sheets[sheet_names[\u001B[38;5;241m0\u001B[39m]]\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Adjust these column names if your Excel file uses different names.\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m table_names \u001B[38;5;241m=\u001B[39m tables_overview_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmasked_table_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'sheets' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158465>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 2. Process the tables overview (first sheet)\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m tables_overview_df \u001B[38;5;241m=\u001B[39m sheets[sheet_names[\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Adjust these column names if your Excel file uses different names.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m table_names \u001B[38;5;241m=\u001B[39m tables_overview_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmasked_table_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\n\u001B[0;31mNameError\u001B[0m: name 'sheets' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'sheets' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 2. Process the tables overview (first sheet)\n",
    "# -------------------------------------\n",
    "# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\n",
    "tables_overview_df = sheets[sheet_names[0]]\n",
    "# Adjust these column names if your Excel file uses different names.\n",
    "table_names = tables_overview_df[\"masked_table_id\"].tolist()\n",
    "approx_row_counts = tables_overview_df[\"num_rows_approx\"].tolist()\n",
    "\n",
    "print(\"Tables and approximate row counts:\")\n",
    "for tbl, cnt in zip(table_names, approx_row_counts):\n",
    "    print(f\"  {tbl}: ~{cnt} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d419e44-8ad0-4faf-9792-3e5111c3421c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158466>:6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 3. Read each table's metadata (columns, types, etc.)\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Here we assume that the sheet name for each table is the same as the table name.\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m table_metadata \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tbl \u001B[38;5;129;01min\u001B[39;00m table_names:\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tbl \u001B[38;5;129;01min\u001B[39;00m sheets:\n",
       "\u001B[1;32m      8\u001B[0m         meta_df \u001B[38;5;241m=\u001B[39m sheets[tbl]\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158466>:6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 3. Read each table's metadata (columns, types, etc.)\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Here we assume that the sheet name for each table is the same as the table name.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m table_metadata \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tbl \u001B[38;5;129;01min\u001B[39;00m table_names:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tbl \u001B[38;5;129;01min\u001B[39;00m sheets:\n\u001B[1;32m      8\u001B[0m         meta_df \u001B[38;5;241m=\u001B[39m sheets[tbl]\n\n\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'table_names' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 3. Read each table's metadata (columns, types, etc.)\n",
    "# -------------------------------------\n",
    "# Here we assume that the sheet name for each table is the same as the table name.\n",
    "table_metadata = {}\n",
    "for tbl in table_names:\n",
    "    if tbl in sheets:\n",
    "        meta_df = sheets[tbl]\n",
    "        table_metadata[tbl] = meta_df\n",
    "        print(f\"Loaded metadata for table '{tbl}'.\")\n",
    "    else:\n",
    "        print(f\"Warning: No metadata sheet found for table '{tbl}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0a6c33-84f3-4c34-8366-8e1f79d59343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 4. Define a mapping from your Excel type names to Spark types.\n",
    "# -------------------------------------\n",
    "spark_type_mapping = {\n",
    "    \"StringType()\": StringType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"LongType()\": LongType(),\n",
    "    \"FloatType()\": FloatType(),\n",
    "    \"DoubleType()\": DoubleType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"DateType()\": DateType(),\n",
    "    \"TimestampType()\": TimestampType(),\n",
    "    \"ArrayType(IntegerType(), True)\": ArrayType(IntegerType(), True),\n",
    "    \"ArrayType(StringType(), True)\": ArrayType(StringType(), True)\n",
    "}\n",
    "\n",
    "def create_schema(meta_df):\n",
    "    \"\"\"\n",
    "    Create a Spark schema (StructType) from the metadata DataFrame.\n",
    "    For numerical types, if \"min\" and \"max\" are provided, they are stored in the field metadata.\n",
    "    This version ensures that the type from the spreadsheet is used (if it matches).\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    # Ensure that the range columns exist in the DataFrame.\n",
    "    has_range = (\"min\" in meta_df.columns) and (\"max\" in meta_df.columns)\n",
    "    \n",
    "    for idx, row in meta_df.iterrows():\n",
    "        col_name = row[\"masked_column_name\"]\n",
    "        # Convert the Type from the spreadsheet to a lower-case string.\n",
    "        type_str = str(row[\"spark_data_type\"]).strip() if pd.notna(row[\"spark_data_type\"]) else \"string\"\n",
    "        spark_type = spark_type_mapping.get(type_str)\n",
    "        \n",
    "        if spark_type is None:\n",
    "            # If the type is not recognized, warn and default to StringType.\n",
    "            print(f\"Warning: Unrecognized type '{row['spark_data_type']}' for column '{col_name}'. Using StringType.\")\n",
    "            spark_type = StringType()\n",
    "        \n",
    "        md = {}\n",
    "        # For numerical types, if min and max values are provided, store them in metadata.\n",
    "        if isinstance(spark_type, (IntegerType, LongType, FloatType, DoubleType)) and has_range:\n",
    "            if pd.notna(row[\"min\"]) and pd.notna(row[\"max\"]):\n",
    "                md[\"min\"] = row[\"min\"]\n",
    "                md[\"max\"] = row[\"max\"]\n",
    "        \n",
    "        fields.append(StructField(col_name, spark_type, True, metadata=md))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "# Create a dictionary of schemas for each table.\n",
    "schemas = {}\n",
    "for tbl, meta_df in table_metadata.items():\n",
    "    schema = create_schema(meta_df)\n",
    "    schemas[tbl] = schema\n",
    "    print(f\"Schema for table '{tbl}': {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a75abc-0d94-4c68-af52-03eefee5d730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158468>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 5. Process join information.\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m join_info_df \u001B[38;5;241m=\u001B[39m sheets[sheet_names[\u001B[38;5;241m1\u001B[39m]]\n",
       "\u001B[1;32m      6\u001B[0m joins \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'sheets' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158468>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Step 5. Process join information.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m join_info_df \u001B[38;5;241m=\u001B[39m sheets[sheet_names[\u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m      6\u001B[0m joins \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\u001B[39;00m\n\n\u001B[0;31mNameError\u001B[0m: name 'sheets' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'sheets' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 5. Process join information.\n",
    "# -------------------------------------\n",
    "# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\n",
    "join_info_df = sheets[sheet_names[1]]\n",
    "joins = []\n",
    "# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\n",
    "for idx, row in join_info_df.iterrows():\n",
    "    join_detail = {\n",
    "        \"left_table\": row[\"table1\"],\n",
    "        \"right_table\": row[\"table2\"],\n",
    "        \"join_method\": row[\"join_method\"],\n",
    "        \"left_column\": row[\"column1\"],\n",
    "        \"right_column\": row[\"column2\"]\n",
    "    }\n",
    "    joins.append(join_detail)\n",
    "\n",
    "print(\"Join definitions:\")\n",
    "for join in joins:\n",
    "    print(f\"  {join['left_table']}.{join['left_column']} {join['join_method'].upper()} JOIN {join['right_table']}.{join['right_column']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e54280-2160-4d6d-97aa-88d2096db7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158469>:11\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# (Optional) Step 6. Build a dynamic join query.\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# The code below builds a join SQL string assuming sequential joining.\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtable_names\u001B[49m:\n",
       "\u001B[1;32m     12\u001B[0m     join_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_names[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m join \u001B[38;5;129;01min\u001B[39;00m joins:\n",
       "\u001B[1;32m     14\u001B[0m         \u001B[38;5;66;03m# Note: This simple logic assumes that the join order is appropriate.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158469>:11\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# (Optional) Step 6. Build a dynamic join query.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# -------------------------------------\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# The code below builds a join SQL string assuming sequential joining.\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtable_names\u001B[49m:\n\u001B[1;32m     12\u001B[0m     join_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_names[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m join \u001B[38;5;129;01min\u001B[39;00m joins:\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;66;03m# Note: This simple logic assumes that the join order is appropriate.\u001B[39;00m\n\n\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'table_names' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# (Optional) Step 6. Build a dynamic join query.\n",
    "# -------------------------------------\n",
    "# If you later load your data into Spark DataFrames and register them as temporary views,\n",
    "# you could build and execute a join query dynamically. For example, suppose you have:\n",
    "#    df_customers = spark.read.csv(\"customers.csv\", schema=schemas[\"customers\"], header=True)\n",
    "#    df_customers.createOrReplaceTempView(\"customers\")\n",
    "#    ... and similarly for other tables.\n",
    "#\n",
    "# The code below builds a join SQL string assuming sequential joining.\n",
    "if table_names:\n",
    "    join_query = f\"SELECT * FROM {table_names[0]}\"\n",
    "    for join in joins:\n",
    "        # Note: This simple logic assumes that the join order is appropriate.\n",
    "        join_query += (\n",
    "            f\" {join['join_method'].upper()} JOIN {join['right_table']} \"\n",
    "            f\"ON {join['left_table']}.{join['left_column']} = {join['right_table']}.{join['right_column']}\"\n",
    "        )\n",
    "    print(\"Constructed join query:\")\n",
    "    print(join_query)\n",
    "    # To execute the query once the tables are registered as temp views:\n",
    "    # result_df = spark.sql(join_query)\n",
    "    # result_df.show()\n",
    "\n",
    "# -------------------------------------\n",
    "# Now you have:\n",
    "#  - 'table_names' and 'approx_row_counts' from the overview.\n",
    "#  - 'table_metadata': a dictionary mapping table names to their metadata DataFrames.\n",
    "#  - 'schemas': a dictionary mapping table names to Spark schemas.\n",
    "#  - 'joins': a list of dictionaries describing the join relationships.\n",
    "#\n",
    "# You can now use this information to drive your Spark ETL/processing workflow.\n",
    "#\n",
    "# When finished, stop the Spark session (if running in a script).\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a369a5-f517-4f7f-b051-45b9fcef8119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Actually Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbb405f-fd8b-42d3-ab31-f479100d0ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158471>:66\u001B[0m\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Create and register a DataFrame for each table using the distributed random data generation.\u001B[39;00m\n",
       "\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\u001B[39;00m\n",
       "\u001B[1;32m     65\u001B[0m dfs \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tbl, count \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(table_names, approx_row_counts):\n",
       "\u001B[1;32m     67\u001B[0m     schema \u001B[38;5;241m=\u001B[39m schemas[tbl]\n",
       "\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tbl \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158471>:66\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Create and register a DataFrame for each table using the distributed random data generation.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\u001B[39;00m\n\u001B[1;32m     65\u001B[0m dfs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tbl, count \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(table_names, approx_row_counts):\n\u001B[1;32m     67\u001B[0m     schema \u001B[38;5;241m=\u001B[39m schemas[tbl]\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tbl \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\n\u001B[0;31mNameError\u001B[0m: name 'table_names' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'table_names' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================================\n",
    "# PART 2: Generate random data for each table and register as temp views\n",
    "# ========================================\n",
    "\n",
    "def generate_random_dataframe(schema, num_rows):\n",
    "    \"\"\"\n",
    "    Given a Spark StructType schema and a number of rows, generate a DataFrame with random data\n",
    "    using Spark’s distributed operations.\n",
    "    For numerical types, if metadata has \"min\" and \"max\", those bounds are used.\n",
    "    \"\"\"\n",
    "    # Start with a DataFrame with a column \"id\" (this DataFrame is generated in a distributed fashion)\n",
    "    df = spark.range(num_rows)\n",
    "    \n",
    "    # For each field in the schema, add a column with a random value.\n",
    "    for field in schema.fields:\n",
    "        col_name = field.name\n",
    "        dt = field.dataType\n",
    "        md = field.metadata or {}\n",
    "        \n",
    "        if isinstance(dt, (IntegerType, LongType)):\n",
    "            # Use provided min and max if available; otherwise default to 1 and 1000.\n",
    "            min_val = md.get(\"min\", 1)\n",
    "            max_val = md.get(\"max\", 1000)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            # Cast appropriately.\n",
    "            if isinstance(dt, IntegerType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"int\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"long\"))\n",
    "                \n",
    "        elif isinstance(dt, (FloatType, DoubleType)):\n",
    "            min_val = md.get(\"min\", 0.0)\n",
    "            max_val = md.get(\"max\", 1000.0)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            if isinstance(dt, FloatType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"float\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"double\"))\n",
    "                \n",
    "        elif isinstance(dt, BooleanType):\n",
    "            # Generate a boolean value based on a threshold.\n",
    "            df = df.withColumn(col_name, F.rand() > 0.5)\n",
    "            \n",
    "        elif isinstance(dt, DateType):\n",
    "            # Generate a random date by adding a random number of days (e.g., 0 to 9000) to a base date.\n",
    "            df = df.withColumn(col_name, F.expr(\"date_add('2000-01-01', cast(rand() * 9000 as int))\"))\n",
    "            \n",
    "        elif isinstance(dt, TimestampType):\n",
    "            # Generate a random timestamp by first generating a random date and then converting it.\n",
    "            df = df.withColumn(col_name, F.expr(\"to_timestamp(date_add('2000-01-01', cast(rand() * 9000 as int)))\"))\n",
    "            \n",
    "        elif isinstance(dt, StringType):\n",
    "            # Use the built-in uuid() function for random strings.\n",
    "            df = df.withColumn(col_name, F.expr(\"uuid()\"))\n",
    "            \n",
    "        else:\n",
    "            # For any unrecognized type, set the column to null.\n",
    "            df = df.withColumn(col_name, F.lit(None))\n",
    "            \n",
    "    # Drop the original \"id\" column.\n",
    "    return df.drop(\"id\")\n",
    "\n",
    "# Create and register a DataFrame for each table using the distributed random data generation.\n",
    "# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\n",
    "dfs = {}\n",
    "for tbl, count in zip(table_names, approx_row_counts):\n",
    "    schema = schemas[tbl]\n",
    "    if tbl == 'table_a':\n",
    "        num_rows = 100000000\n",
    "    elif tbl == 'table_c':\n",
    "        num_rows = 21000000\n",
    "    else:\n",
    "        num_rows = int(count)\n",
    "    # num_rows = int(count)\n",
    "    df = generate_random_dataframe(schema, num_rows)\n",
    "    dfs[tbl] = df\n",
    "    print(f\"Created DataFrame for table '{tbl}' with {num_rows} random rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1945eb13-1f2f-4de6-ae17-86d261932513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GroupBys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f53375a-a78f-4275-8651-3f74442844a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158473>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m table_a \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m      2\u001B[0m gb_test1 \u001B[38;5;241m=\u001B[39m table_a\u001B[38;5;241m.\u001B[39mgroupBy([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_5\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_7\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_9\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcount()\n",
       "\u001B[1;32m      4\u001B[0m gb_test1\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnoop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'table_a'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158473>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m table_a \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      2\u001B[0m gb_test1 \u001B[38;5;241m=\u001B[39m table_a\u001B[38;5;241m.\u001B[39mgroupBy([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_5\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_7\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_a_9\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m      4\u001B[0m gb_test1\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnoop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mKeyError\u001B[0m: 'table_a'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'table_a'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test1 = table_a.groupBy(['col_a_1', 'col_a_3', 'col_a_5', 'col_a_7', 'col_a_9']).count()\n",
    "\n",
    "gb_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d734fac-d3b0-4cf3-bc2f-e3e837cc89e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test2 = table_a.groupBy(['col_a_1']).count()\n",
    "\n",
    "gb_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b983468b-a711-4502-ade9-544f2f24e894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7aae09-ab6a-427c-bbd5-4abe614648a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### table_c `col_1` and `col_3` are both ArrayType(IntegerType) columns in real life, most of the time the array is one value long, but sometimes it's multiple values. Try and figure this out at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb51607-cddb-48be-a4a9-de6da2816eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_b = dfs['table_b']\n",
    "\n",
    "join_test1 = table_a.join(table_b, [\n",
    "    table_a[\"col_a_1\"]==table_b[\"col_b_8\"],\n",
    "    table_a[\"col_a_3\"]==table_b[\"col_b_3\"],\n",
    "    table_a[\"col_a_5\"]==table_b[\"col_b_9\"],\n",
    "    table_a[\"col_a_7\"]==table_b[\"col_b_1\"],\n",
    "],\n",
    "how='left')\n",
    "join_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba0b929-c3d6-43f6-a18a-31039e496c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_c = dfs['table_c']\n",
    "\n",
    "join_test2 = table_a.join(table_c, [\n",
    "    table_a[\"col_a_1\"]==table_c[\"col_c_10\"],\n",
    "    table_a[\"col_a_3\"]==table_c[\"col_c_9\"],\n",
    "    table_a[\"col_a_9\"]==table_c[\"col_c_11\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e02e0c4-5c04-467e-b533-b345b308a18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_d = dfs['table_d']\n",
    "\n",
    "join_test3 = table_a.join(table_d, [\n",
    "    table_a[\"col_a_1\"]==table_d[\"col_d_0\"],\n",
    "    table_a[\"col_a_5\"]==table_d[\"col_d_1\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test3.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5824fe-5fae-4f63-8dd9-117b185489e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test4 = table_a.join(table_e, table_a[\"col_a_1\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test4.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0635d3-d4ba-4760-aa23-38fcca5730dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test5 = table_c.join(table_e, table_c[\"col_c_5\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test5.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085d4042-5da3-43ec-b0f9-db94acec7e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test6 = table_c.join(table_e, table_c[\"col_c_10\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test6.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17eab97-b90d-4f91-b6c3-fc95b07b38e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "linked_join_test = (\n",
    "    table_a\n",
    "    .join(\n",
    "        table_b,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_b[\"col_b_8\"],\n",
    "            table_a[\"col_a_3\"] == table_b[\"col_b_3\"],\n",
    "            table_a[\"col_a_5\"] == table_b[\"col_b_9\"],\n",
    "            table_a[\"col_a_7\"] == table_b[\"col_b_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_c,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_c[\"col_c_10\"],\n",
    "            table_a[\"col_a_3\"] == table_c[\"col_c_9\"],\n",
    "            table_a[\"col_a_9\"] == table_c[\"col_c_11\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_d,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_d[\"col_d_0\"],\n",
    "            table_a[\"col_a_5\"] == table_d[\"col_d_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_e,\n",
    "        table_a[\"col_a_1\"] == table_e[\"col_e_0\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "linked_join_test.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "341579d7-066a-4f80-949a-61ad62ae6ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Breadth First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfac22c-2ca2-457f-a7c6-96d83fe4d5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2697433332302766>:9\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functions \u001B[38;5;28;01mas\u001B[39;00m F\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create or get your Spark session\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# spark = SparkSession.builder.getOrCreate()\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Assume your input DataFrame 'df' has columns \"col_0\" and \"col_1\".\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# We create an 'edges' DataFrame with \"src\" and \"dst\" columns.\u001B[39;00m\n",
       "\u001B[0;32m----> 9\u001B[0m df \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m100000\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m edges \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_a_3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_a_7\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdst\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Define the BFS starting point.\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Change the 'source' variable to the vertex from which you want to start the BFS.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'table_a'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2697433332302766>:9\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functions \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create or get your Spark session\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# spark = SparkSession.builder.getOrCreate()\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Assume your input DataFrame 'df' has columns \"col_0\" and \"col_1\".\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# We create an 'edges' DataFrame with \"src\" and \"dst\" columns.\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m df \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m100000\u001B[39m)\n\u001B[1;32m     11\u001B[0m edges \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_a_3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_a_7\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdst\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Define the BFS starting point.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Change the 'source' variable to the vertex from which you want to start the BFS.\u001B[39;00m\n\n\u001B[0;31mKeyError\u001B[0m: 'table_a'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'table_a'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assume your input DataFrame 'df' has columns \"col_0\" and \"col_1\".\n",
    "# We create an 'edges' DataFrame with \"src\" and \"dst\" columns.\n",
    "df = dfs['table_a'].limit(100000)\n",
    "\n",
    "edges = df.select(F.col(\"col_a_3\").alias(\"src\"), F.col(\"col_a_7\").alias(\"dst\"))\n",
    "\n",
    "# Define the BFS starting point.\n",
    "# Change the 'source' variable to the vertex from which you want to start the BFS.\n",
    "source = 1000  # For example, use \"A\" as the starting vertex\n",
    "\n",
    "# Create the initial frontier: the source vertex with distance 0.\n",
    "frontier = spark.createDataFrame([(source, 0)], [\"vertex\", \"distance\"])\n",
    "\n",
    "# Create a DataFrame to keep track of all visited vertices (and their distance from the source).\n",
    "visited = frontier\n",
    "\n",
    "# Loop until there are no new nodes to visit.\n",
    "while frontier.count() > 0:\n",
    "    # 1. Find neighbors: join the current frontier with the edges DataFrame.\n",
    "    #    Each neighbor gets a distance equal to (current distance + 1).\n",
    "    new_neighbors = frontier.join(edges, frontier.vertex == edges.src) \\\n",
    "                            .select(edges.dst.alias(\"vertex\"),\n",
    "                                    (frontier.distance + 1).alias(\"distance\"))\n",
    "    \n",
    "    # 2. Exclude vertices that have already been visited.\n",
    "    new_neighbors = new_neighbors.join(visited, on=\"vertex\", how=\"left_anti\").distinct()\n",
    "    \n",
    "    # 3. If no new vertices are found, exit the loop.\n",
    "    if new_neighbors.count() == 0:\n",
    "        break\n",
    "    \n",
    "    # 4. Add the new neighbors to the visited set.\n",
    "    visited = visited.union(new_neighbors).distinct()\n",
    "    \n",
    "    # 5. Update the frontier to be the new neighbors.\n",
    "    frontier = new_neighbors\n",
    "\n",
    "# The 'visited' DataFrame now contains all vertices reachable from the source,\n",
    "# along with the minimum number of steps (distance) from the source.\n",
    "visited.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dadef6dc-8218-469f-a662-8989e8696f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2ba73a-c61f-4379-b000-fad9d74e9a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2697433332302768>:7\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functions \u001B[38;5;28;01mas\u001B[39;00m F\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create or get your Spark session\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# spark = SparkSession.builder.getOrCreate()\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m table_a \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m      8\u001B[0m df \u001B[38;5;241m=\u001B[39m table_a\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m10000\u001B[39m) \u001B[38;5;66;03m# Change as needed\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Set the reset (teleportation) probability and the number of iterations\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'table_a'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2697433332302768>:7\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functions \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create or get your Spark session\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# spark = SparkSession.builder.getOrCreate()\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m table_a \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      8\u001B[0m df \u001B[38;5;241m=\u001B[39m table_a\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m10000\u001B[39m) \u001B[38;5;66;03m# Change as needed\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Set the reset (teleportation) probability and the number of iterations\u001B[39;00m\n\n\u001B[0;31mKeyError\u001B[0m: 'table_a'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'table_a'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "table_a = dfs['table_a']\n",
    "df = table_a.limit(10000) # Change as needed\n",
    "\n",
    "# Set the reset (teleportation) probability and the number of iterations\n",
    "alpha = 0.15\n",
    "maxIter = 10\n",
    "\n",
    "# Choose the personalized seed: take the first value from col_a_3\n",
    "seed = df.select(\"col_a_3\").first()[0]\n",
    "\n",
    "# 1. Create the vertices DataFrame: union of unique IDs from col_a_3 and col_a_7.\n",
    "vertices = (\n",
    "    df.select(F.col(\"col_a_3\").alias(\"id\"))\n",
    "      .union(df.select(F.col(\"col_a_7\").alias(\"id\")))\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# 2. Create the edges DataFrame: define edge from col_a_3 to col_a_7.\n",
    "edges = df.select(F.col(\"col_a_3\").alias(\"src\"), F.col(\"col_a_7\").alias(\"dst\"))\n",
    "\n",
    "# 3. Compute out-degrees: count of outgoing edges for each source vertex.\n",
    "out_degrees = edges.groupBy(\"src\").agg(F.count(\"*\").alias(\"out_degree\"))\n",
    "\n",
    "# 4. Initialize each vertex with a PageRank value:\n",
    "#    The seed gets 1.0 and all others start with 0.0.\n",
    "vertices_rank = vertices.withColumn(\n",
    "    \"rank\", F.when(F.col(\"id\") == seed, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# 5. Iteratively update the PageRank values.\n",
    "for i in range(maxIter):\n",
    "    # 5a. For each edge, compute the contribution from its source.\n",
    "    #     Join the edges with the current vertex ranks and the out-degrees.\n",
    "    contribs = (\n",
    "        edges.join(vertices_rank, edges.src == vertices_rank.id)\n",
    "             .join(out_degrees, edges.src == out_degrees.src)\n",
    "             .select(\n",
    "                 edges.dst.alias(\"id\"),\n",
    "                 (vertices_rank.rank / out_degrees.out_degree).alias(\"contrib\")\n",
    "             )\n",
    "    )\n",
    "    \n",
    "    # 5b. Sum the contributions arriving at each vertex.\n",
    "    contribs_sum = contribs.groupBy(\"id\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "    \n",
    "    # 5c. Compute the total rank from dangling nodes (vertices with no outgoing edges).\n",
    "    dangling = (\n",
    "        vertices_rank.join(out_degrees, vertices_rank.id == out_degrees.src, \"left\")\n",
    "                     .withColumn(\"out_degree\", F.coalesce(F.col(\"out_degree\"), F.lit(0)))\n",
    "                     .where(F.col(\"out_degree\") == 0)\n",
    "    )\n",
    "    dangling_sum = dangling.agg(F.sum(\"rank\").alias(\"dangling_sum\")).collect()[0][\"dangling_sum\"]\n",
    "    if dangling_sum is None:\n",
    "        dangling_sum = 0.0\n",
    "    \n",
    "    # 5d. Update each vertex's rank:\n",
    "    #     - If the vertex is the seed, it gets the reset term (alpha)\n",
    "    #       plus (1 - alpha) times (its incoming contributions plus dangling rank).\n",
    "    #     - Otherwise, it just gets (1 - alpha) times its incoming contributions.\n",
    "    vertices_rank = (\n",
    "        vertices.join(contribs_sum, on=\"id\", how=\"left\")\n",
    "                .na.fill({\"sum_contrib\": 0.0})\n",
    "                .withColumn(\"rank\", \n",
    "                    F.when(F.col(\"id\") == seed,\n",
    "                           alpha + (1 - alpha) * (F.col(\"sum_contrib\") + dangling_sum)\n",
    "                    ).otherwise(\n",
    "                           (1 - alpha) * F.col(\"sum_contrib\")\n",
    "                    )\n",
    "                )\n",
    "    )\n",
    "\n",
    "# 6. Display the final personalized PageRank values.\n",
    "vertices_rank.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d45fbc-6044-484e-986c-4b693b830f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Barabasi-Albert Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3c6907-d879-4314-8dfb-ed305cfba507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2280788391158487>:71\u001B[0m\n",
       "\u001B[1;32m     68\u001B[0m m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m                \u001B[38;5;66;03m# Each new node connects to m existing nodes\u001B[39;00m\n",
       "\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Generate the BA graph edges with five-tuple data\u001B[39;00m\n",
       "\u001B[0;32m---> 71\u001B[0m edges \u001B[38;5;241m=\u001B[39m generate_ba_graph(total_nodes, m)\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Define a schema for the edges DataFrame\u001B[39;00m\n",
       "\u001B[1;32m     74\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m     75\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc_node\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n",
       "\u001B[1;32m     76\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdst_node\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     81\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_4\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     82\u001B[0m ])\n",
       "\n",
       "File \u001B[0;32m<command-2280788391158487>:33\u001B[0m, in \u001B[0;36mgenerate_ba_graph\u001B[0;34m(total_nodes, m)\u001B[0m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(m):\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, m):\n",
       "\u001B[0;32m---> 33\u001B[0m         five_t \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_five_t\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     34\u001B[0m         edges\u001B[38;5;241m.\u001B[39mappend((i, j,\n",
       "\u001B[1;32m     35\u001B[0m                       five_t[\u001B[38;5;241m0\u001B[39m], five_t[\u001B[38;5;241m1\u001B[39m],\n",
       "\u001B[1;32m     36\u001B[0m                       five_t[\u001B[38;5;241m2\u001B[39m], five_t[\u001B[38;5;241m3\u001B[39m],\n",
       "\u001B[1;32m     37\u001B[0m                       five_t[\u001B[38;5;241m4\u001B[39m]))\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Step 2: Maintain a list of nodes repeated by their degree.\u001B[39;00m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# This list will be used for preferential attachment.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: generate_five_t() takes 1 positional argument but 2 were given"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2280788391158487>:71\u001B[0m\n\u001B[1;32m     68\u001B[0m m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m                \u001B[38;5;66;03m# Each new node connects to m existing nodes\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Generate the BA graph edges with five-tuple data\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m edges \u001B[38;5;241m=\u001B[39m generate_ba_graph(total_nodes, m)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Define a schema for the edges DataFrame\u001B[39;00m\n\u001B[1;32m     74\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     75\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc_node\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     76\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdst_node\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     81\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol_4\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     82\u001B[0m ])\n\nFile \u001B[0;32m<command-2280788391158487>:33\u001B[0m, in \u001B[0;36mgenerate_ba_graph\u001B[0;34m(total_nodes, m)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(m):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, m):\n\u001B[0;32m---> 33\u001B[0m         five_t \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_five_t\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m         edges\u001B[38;5;241m.\u001B[39mappend((i, j,\n\u001B[1;32m     35\u001B[0m                       five_t[\u001B[38;5;241m0\u001B[39m], five_t[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     36\u001B[0m                       five_t[\u001B[38;5;241m2\u001B[39m], five_t[\u001B[38;5;241m3\u001B[39m],\n\u001B[1;32m     37\u001B[0m                       five_t[\u001B[38;5;241m4\u001B[39m]))\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Step 2: Maintain a list of nodes repeated by their degree.\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# This list will be used for preferential attachment.\u001B[39;00m\n\n\u001B[0;31mTypeError\u001B[0m: generate_five_t() takes 1 positional argument but 2 were given",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: generate_five_t() takes 1 positional argument but 2 were given",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import random\n",
    "\n",
    "# def generate_ip(node_id):\n",
    "def generate_nodes(node_id):\n",
    "    return f\"192.168.{node_id // 256}.{node_id % 256}\"\n",
    "\n",
    "# def generate_base_nodes(node_id):\n",
    "def generate_five_t(node_id):\n",
    "    col_0 = generate_nodes(src)\n",
    "    col_1 = generate_nodes(dst)\n",
    "    col_2 = random.randint(1000, 70000)\n",
    "    col_3 = random.randint(1000, 70000)\n",
    "    col_4 = random.choice([\"AAA\", \"BBB\"])\n",
    "    return (col_0, col_1, col_2, col_3, col_4)\n",
    "\n",
    "def generate_ba_graph(total_nodes, m):\n",
    "    \"\"\"\n",
    "    Generate a graph using the Barabási–Albert (BA) model.\n",
    "    - total_nodes: Total number of nodes in the graph.\n",
    "    - m: Number of edges each new node will attach to.\n",
    "    \n",
    "    Returns a list of edges.\n",
    "    Each edge is represented as a tuple:\n",
    "      (src_node, dst_node, col_0, col_1, col_2, col_3, col_4)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    # Step 1: Create an initial complete graph among the first m nodes.\n",
    "    initial_nodes = list(range(m))\n",
    "    for i in range(m):\n",
    "        for j in range(i + 1, m):\n",
    "            five_t = generate_five_t(i, j)\n",
    "            edges.append((i, j,\n",
    "                          five_t[0], five_t[1],\n",
    "                          five_t[2], five_t[3],\n",
    "                          five_t[4]))\n",
    "    \n",
    "    # Step 2: Maintain a list of nodes repeated by their degree.\n",
    "    # This list will be used for preferential attachment.\n",
    "    repeated_nodes = []\n",
    "    # In the complete graph, each node has degree (m - 1)\n",
    "    for node in initial_nodes:\n",
    "        repeated_nodes.extend([node] * (m - 1))\n",
    "    \n",
    "    # Step 3: Iteratively add new nodes to the graph.\n",
    "    for new_node in range(m, total_nodes):\n",
    "        targets = set()\n",
    "        # Sample m unique target nodes with probability proportional to degree.\n",
    "        while len(targets) < m:\n",
    "            target = random.choice(repeated_nodes)\n",
    "            targets.add(target)\n",
    "        for target in targets:\n",
    "            five_tuple = generate_five_tuple(new_node, target)\n",
    "            edges.append((new_node, target,\n",
    "                          five_tuple[0], five_tuple[1],\n",
    "                          five_tuple[2], five_tuple[3],\n",
    "                          five_tuple[4]))\n",
    "            # Update the repeated_nodes list:\n",
    "            repeated_nodes.append(new_node)\n",
    "            repeated_nodes.append(target)\n",
    "    return edges\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parameters for BA graph\n",
    "    total_nodes = 1_000_000   # Adjust as needed\n",
    "    m = 3                # Each new node connects to m existing nodes\n",
    "\n",
    "    # Generate the BA graph edges with five-tuple data\n",
    "    edges = generate_ba_graph(total_nodes, m)\n",
    "\n",
    "    # Define a schema for the edges DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"src_node\", IntegerType(), False),\n",
    "        StructField(\"dst_node\", IntegerType(), False),\n",
    "        StructField(\"col_0\", StringType(), False),\n",
    "        StructField(\"col_1\", StringType(), False),\n",
    "        StructField(\"col_2\", IntegerType(), False),\n",
    "        StructField(\"col_3\", IntegerType(), False),\n",
    "        StructField(\"col_4\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # Create a DataFrame from the edge list\n",
    "    edges_df = spark.createDataFrame(edges, schema)\n",
    "    # edges_df.cache()\n",
    "\n",
    "    # Show a few rows of the generated graph\n",
    "    # edges_df.show(10, truncate=False)\n",
    "    edges_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "    # Optionally, write the graph data to disk (e.g., CSV)\n",
    "    # edges_df.write.csv(\"ba_graph_output.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97898f96-fb3d-46e1-8c36-83c0f1410dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1564833091230482,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) HPE_NVDA_TEST",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
