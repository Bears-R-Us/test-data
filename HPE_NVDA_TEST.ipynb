{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c271641c-fb06-4735-a64d-de5a4670d1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, BooleanType, DateType, TimestampType, ArrayType\n",
    ")\n",
    "# Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ExcelMetadataWorkflow\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb5f91a-4074-458b-8227-b2db8a003e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171fc0c9-c0b0-4609-875e-edd205561ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 1. Read all sheets from the Excel file.\n",
    "# -------------------------------------\n",
    "excel_path = \"/tmp/HPE_NVDA_datagen.xlsx\"  # update this path\n",
    "\n",
    "# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "# sheets = spark.read.  \n",
    "sheet_names = list(sheets.keys())\n",
    "print(\"Found sheets:\", sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834dbb3a-fefd-4d82-941b-0f3afb06dc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 2. Process the tables overview (first sheet)\n",
    "# -------------------------------------\n",
    "# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\n",
    "tables_overview_df = sheets[sheet_names[0]]\n",
    "# Adjust these column names if your Excel file uses different names.\n",
    "table_names = tables_overview_df[\"masked_table_id\"].tolist()\n",
    "approx_row_counts = tables_overview_df[\"num_rows_approx\"].tolist()\n",
    "\n",
    "print(\"Tables and approximate row counts:\")\n",
    "for tbl, cnt in zip(table_names, approx_row_counts):\n",
    "    print(f\"  {tbl}: ~{cnt} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d419e44-8ad0-4faf-9792-3e5111c3421c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 3. Read each table's metadata (columns, types, etc.)\n",
    "# -------------------------------------\n",
    "# Here we assume that the sheet name for each table is the same as the table name.\n",
    "table_metadata = {}\n",
    "for tbl in table_names:\n",
    "    if tbl in sheets:\n",
    "        meta_df = sheets[tbl]\n",
    "        table_metadata[tbl] = meta_df\n",
    "        print(f\"Loaded metadata for table '{tbl}'.\")\n",
    "    else:\n",
    "        print(f\"Warning: No metadata sheet found for table '{tbl}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0a6c33-84f3-4c34-8366-8e1f79d59343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 4. Define a mapping from your Excel type names to Spark types.\n",
    "# -------------------------------------\n",
    "spark_type_mapping = {\n",
    "    \"StringType()\": StringType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"LongType()\": LongType(),\n",
    "    \"FloatType()\": FloatType(),\n",
    "    \"DoubleType()\": DoubleType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"DateType()\": DateType(),\n",
    "    \"TimestampType()\": TimestampType(),\n",
    "    \"ArrayType(IntegerType(), True)\": ArrayType(IntegerType(), True),\n",
    "    \"ArrayType(StringType(), True)\": ArrayType(StringType(), True)\n",
    "}\n",
    "\n",
    "def create_schema(meta_df):\n",
    "    \"\"\"\n",
    "    Create a Spark schema (StructType) from the metadata DataFrame.\n",
    "    For numerical types, if \"min\" and \"max\" are provided, they are stored in the field metadata.\n",
    "    This version ensures that the type from the spreadsheet is used (if it matches).\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    # Ensure that the range columns exist in the DataFrame.\n",
    "    has_range = (\"min\" in meta_df.columns) and (\"max\" in meta_df.columns)\n",
    "    \n",
    "    for idx, row in meta_df.iterrows():\n",
    "        col_name = row[\"masked_column_name\"]\n",
    "        # Convert the Type from the spreadsheet to a lower-case string.\n",
    "        type_str = str(row[\"spark_data_type\"]).strip() if pd.notna(row[\"spark_data_type\"]) else \"string\"\n",
    "        spark_type = spark_type_mapping.get(type_str)\n",
    "        \n",
    "        if spark_type is None:\n",
    "            # If the type is not recognized, warn and default to StringType.\n",
    "            print(f\"Warning: Unrecognized type '{row['spark_data_type']}' for column '{col_name}'. Using StringType.\")\n",
    "            spark_type = StringType()\n",
    "        \n",
    "        md = {}\n",
    "        # For numerical types, if min and max values are provided, store them in metadata.\n",
    "        if isinstance(spark_type, (IntegerType, LongType, FloatType, DoubleType)) and has_range:\n",
    "            if pd.notna(row[\"min\"]) and pd.notna(row[\"max\"]):\n",
    "                md[\"min\"] = row[\"min\"]\n",
    "                md[\"max\"] = row[\"max\"]\n",
    "        \n",
    "        fields.append(StructField(col_name, spark_type, True, metadata=md))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "# Create a dictionary of schemas for each table.\n",
    "schemas = {}\n",
    "for tbl, meta_df in table_metadata.items():\n",
    "    schema = create_schema(meta_df)\n",
    "    schemas[tbl] = schema\n",
    "    print(f\"Schema for table '{tbl}': {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a75abc-0d94-4c68-af52-03eefee5d730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 5. Process join information.\n",
    "# -------------------------------------\n",
    "# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\n",
    "join_info_df = sheets[sheet_names[1]]\n",
    "joins = []\n",
    "# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\n",
    "for idx, row in join_info_df.iterrows():\n",
    "    join_detail = {\n",
    "        \"left_table\": row[\"table1\"],\n",
    "        \"right_table\": row[\"table2\"],\n",
    "        \"join_method\": row[\"join_method\"],\n",
    "        \"left_column\": row[\"column1\"],\n",
    "        \"right_column\": row[\"column2\"]\n",
    "    }\n",
    "    joins.append(join_detail)\n",
    "\n",
    "print(\"Join definitions:\")\n",
    "for join in joins:\n",
    "    print(f\"  {join['left_table']}.{join['left_column']} {join['join_method'].upper()} JOIN {join['right_table']}.{join['right_column']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e54280-2160-4d6d-97aa-88d2096db7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# (Optional) Step 6. Build a dynamic join query.\n",
    "# -------------------------------------\n",
    "# If you later load your data into Spark DataFrames and register them as temporary views,\n",
    "# you could build and execute a join query dynamically. For example, suppose you have:\n",
    "#    df_customers = spark.read.csv(\"customers.csv\", schema=schemas[\"customers\"], header=True)\n",
    "#    df_customers.createOrReplaceTempView(\"customers\")\n",
    "#    ... and similarly for other tables.\n",
    "#\n",
    "# The code below builds a join SQL string assuming sequential joining.\n",
    "if table_names:\n",
    "    join_query = f\"SELECT * FROM {table_names[0]}\"\n",
    "    for join in joins:\n",
    "        # Note: This simple logic assumes that the join order is appropriate.\n",
    "        join_query += (\n",
    "            f\" {join['join_method'].upper()} JOIN {join['right_table']} \"\n",
    "            f\"ON {join['left_table']}.{join['left_column']} = {join['right_table']}.{join['right_column']}\"\n",
    "        )\n",
    "    print(\"Constructed join query:\")\n",
    "    print(join_query)\n",
    "    # To execute the query once the tables are registered as temp views:\n",
    "    # result_df = spark.sql(join_query)\n",
    "    # result_df.show()\n",
    "\n",
    "# -------------------------------------\n",
    "# Now you have:\n",
    "#  - 'table_names' and 'approx_row_counts' from the overview.\n",
    "#  - 'table_metadata': a dictionary mapping table names to their metadata DataFrames.\n",
    "#  - 'schemas': a dictionary mapping table names to Spark schemas.\n",
    "#  - 'joins': a list of dictionaries describing the join relationships.\n",
    "#\n",
    "# You can now use this information to drive your Spark ETL/processing workflow.\n",
    "#\n",
    "# When finished, stop the Spark session (if running in a script).\n",
    "# spark.stop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a369a5-f517-4f7f-b051-45b9fcef8119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Actually Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbb405f-fd8b-42d3-ab31-f479100d0ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PART 2: Generate random data for each table and register as temp views\n",
    "# ========================================\n",
    "\n",
    "def generate_random_dataframe(schema, num_rows):\n",
    "    \"\"\"\n",
    "    Given a Spark StructType schema and a number of rows, generate a DataFrame with random data\n",
    "    using Spark’s distributed operations.\n",
    "    For numerical types, if metadata has \"min\" and \"max\", those bounds are used.\n",
    "    \"\"\"\n",
    "    # Start with a DataFrame with a column \"id\" (this DataFrame is generated in a distributed fashion)\n",
    "    df = spark.range(num_rows)\n",
    "    \n",
    "    # For each field in the schema, add a column with a random value.\n",
    "    for field in schema.fields:\n",
    "        col_name = field.name\n",
    "        dt = field.dataType\n",
    "        md = field.metadata or {}\n",
    "        \n",
    "        if isinstance(dt, (IntegerType, LongType)):\n",
    "            # Use provided min and max if available; otherwise default to 1 and 1000.\n",
    "            min_val = md.get(\"min\", 1)\n",
    "            max_val = md.get(\"max\", 1000)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            # Cast appropriately.\n",
    "            if isinstance(dt, IntegerType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"int\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"long\"))\n",
    "                \n",
    "        elif isinstance(dt, (FloatType, DoubleType)):\n",
    "            min_val = md.get(\"min\", 0.0)\n",
    "            max_val = md.get(\"max\", 1000.0)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            if isinstance(dt, FloatType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"float\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"double\"))\n",
    "                \n",
    "        elif isinstance(dt, BooleanType):\n",
    "            # Generate a boolean value based on a threshold.\n",
    "            df = df.withColumn(col_name, F.rand() > 0.5)\n",
    "            \n",
    "        elif isinstance(dt, DateType):\n",
    "            # Generate a random date by adding a random number of days (e.g., 0 to 9000) to a base date.\n",
    "            df = df.withColumn(col_name, F.expr(\"date_add('2000-01-01', cast(rand() * 9000 as int))\"))\n",
    "            \n",
    "        elif isinstance(dt, TimestampType):\n",
    "            # Generate a random timestamp by first generating a random date and then converting it.\n",
    "            df = df.withColumn(col_name, F.expr(\"to_timestamp(date_add('2000-01-01', cast(rand() * 9000 as int)))\"))\n",
    "            \n",
    "        elif isinstance(dt, StringType):\n",
    "            # Use the built-in uuid() function for random strings.\n",
    "            df = df.withColumn(col_name, F.expr(\"uuid()\"))\n",
    "            \n",
    "        else:\n",
    "            # For any unrecognized type, set the column to null.\n",
    "            df = df.withColumn(col_name, F.lit(None))\n",
    "            \n",
    "    # Drop the original \"id\" column.\n",
    "    return df.drop(\"id\")\n",
    "\n",
    "# Create and register a DataFrame for each table using the distributed random data generation.\n",
    "# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\n",
    "dfs = {}\n",
    "for tbl, count in zip(table_names, approx_row_counts):\n",
    "    schema = schemas[tbl]\n",
    "    if tbl == 'table_a':\n",
    "        num_rows = 100000000\n",
    "    elif tbl == 'table_c':\n",
    "        num_rows = 21000000\n",
    "    else:\n",
    "        num_rows = int(count)\n",
    "    # num_rows = int(count)\n",
    "    df = generate_random_dataframe(schema, num_rows)\n",
    "    dfs[tbl] = df\n",
    "    print(f\"Created DataFrame for table '{tbl}' with {num_rows} random rows.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1945eb13-1f2f-4de6-ae17-86d261932513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GroupBys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f53375a-a78f-4275-8651-3f74442844a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test1 = table_a.groupBy(['col_a_1', 'col_a_3', 'col_a_5', 'col_a_7', 'col_a_9']).count()\n",
    "\n",
    "gb_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d734fac-d3b0-4cf3-bc2f-e3e837cc89e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test2 = table_a.groupBy(['col_a_1']).count()\n",
    "\n",
    "gb_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b983468b-a711-4502-ade9-544f2f24e894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7aae09-ab6a-427c-bbd5-4abe614648a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### table_c `col_1` and `col_3` are both ArrayType(IntegerType) columns in real life, most of the time the array is one value long, but sometimes it's multiple values. Try and figure this out at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb51607-cddb-48be-a4a9-de6da2816eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_b = dfs['table_b']\n",
    "\n",
    "join_test1 = table_a.join(table_b, [\n",
    "    table_a[\"col_a_1\"]==table_b[\"col_b_8\"],\n",
    "    table_a[\"col_a_3\"]==table_b[\"col_b_3\"],\n",
    "    table_a[\"col_a_5\"]==table_b[\"col_b_9\"],\n",
    "    table_a[\"col_a_7\"]==table_b[\"col_b_1\"],\n",
    "],\n",
    "how='left')\n",
    "join_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba0b929-c3d6-43f6-a18a-31039e496c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_c = dfs['table_c']\n",
    "\n",
    "join_test2 = table_a.join(table_c, [\n",
    "    table_a[\"col_a_1\"]==table_c[\"col_c_10\"],\n",
    "    table_a[\"col_a_3\"]==table_c[\"col_c_9\"],\n",
    "    table_a[\"col_a_9\"]==table_c[\"col_c_11\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e02e0c4-5c04-467e-b533-b345b308a18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_d = dfs['table_d']\n",
    "\n",
    "join_test3 = table_a.join(table_d, [\n",
    "    table_a[\"col_a_1\"]==table_d[\"col_d_0\"],\n",
    "    table_a[\"col_a_5\"]==table_d[\"col_d_1\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test3.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5824fe-5fae-4f63-8dd9-117b185489e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test4 = table_a.join(table_e, table_a[\"col_a_1\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test4.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0635d3-d4ba-4760-aa23-38fcca5730dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test5 = table_c.join(table_e, table_c[\"col_c_5\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test5.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085d4042-5da3-43ec-b0f9-db94acec7e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test6 = table_c.join(table_e, table_c[\"col_c_10\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test6.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17eab97-b90d-4f91-b6c3-fc95b07b38e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "linked_join_test = (\n",
    "    table_a\n",
    "    .join(\n",
    "        table_b,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_b[\"col_b_8\"],\n",
    "            table_a[\"col_a_3\"] == table_b[\"col_b_3\"],\n",
    "            table_a[\"col_a_5\"] == table_b[\"col_b_9\"],\n",
    "            table_a[\"col_a_7\"] == table_b[\"col_b_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_c,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_c[\"col_c_10\"],\n",
    "            table_a[\"col_a_3\"] == table_c[\"col_c_9\"],\n",
    "            table_a[\"col_a_9\"] == table_c[\"col_c_11\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_d,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_d[\"col_d_0\"],\n",
    "            table_a[\"col_a_5\"] == table_d[\"col_d_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_e,\n",
    "        table_a[\"col_a_1\"] == table_e[\"col_e_0\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "linked_join_test.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "341579d7-066a-4f80-949a-61ad62ae6ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Breadth First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfac22c-2ca2-457f-a7c6-96d83fe4d5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assume your input DataFrame 'df' has columns \"col_0\" and \"col_1\".\n",
    "# We create an 'edges' DataFrame with \"src\" and \"dst\" columns.\n",
    "df = dfs['table_a'].limit(100000)\n",
    "\n",
    "edges = df.select(F.col(\"col_a_3\").alias(\"src\"), F.col(\"col_a_7\").alias(\"dst\"))\n",
    "\n",
    "# Define the BFS starting point.\n",
    "# Change the 'source' variable to the vertex from which you want to start the BFS.\n",
    "source = 1000  # For example, use \"A\" as the starting vertex\n",
    "\n",
    "# Create the initial frontier: the source vertex with distance 0.\n",
    "frontier = spark.createDataFrame([(source, 0)], [\"vertex\", \"distance\"])\n",
    "\n",
    "# Create a DataFrame to keep track of all visited vertices (and their distance from the source).\n",
    "visited = frontier\n",
    "\n",
    "# Loop until there are no new nodes to visit.\n",
    "while frontier.count() > 0:\n",
    "    # 1. Find neighbors: join the current frontier with the edges DataFrame.\n",
    "    #    Each neighbor gets a distance equal to (current distance + 1).\n",
    "    new_neighbors = frontier.join(edges, frontier.vertex == edges.src) \\\n",
    "                            .select(edges.dst.alias(\"vertex\"),\n",
    "                                    (frontier.distance + 1).alias(\"distance\"))\n",
    "    \n",
    "    # 2. Exclude vertices that have already been visited.\n",
    "    new_neighbors = new_neighbors.join(visited, on=\"vertex\", how=\"left_anti\").distinct()\n",
    "    \n",
    "    # 3. If no new vertices are found, exit the loop.\n",
    "    if new_neighbors.count() == 0:\n",
    "        break\n",
    "    \n",
    "    # 4. Add the new neighbors to the visited set.\n",
    "    visited = visited.union(new_neighbors).distinct()\n",
    "    \n",
    "    # 5. Update the frontier to be the new neighbors.\n",
    "    frontier = new_neighbors\n",
    "\n",
    "# The 'visited' DataFrame now contains all vertices reachable from the source,\n",
    "# along with the minimum number of steps (distance) from the source.\n",
    "visited.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dadef6dc-8218-469f-a662-8989e8696f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2ba73a-c61f-4379-b000-fad9d74e9a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "table_a = dfs['table_a']\n",
    "df = table_a.limit(10000) # Change as needed\n",
    "\n",
    "# Set the reset (teleportation) probability and the number of iterations\n",
    "alpha = 0.15\n",
    "maxIter = 10\n",
    "\n",
    "# Choose the personalized seed: take the first value from col_a_3\n",
    "seed = df.select(\"col_a_3\").first()[0]\n",
    "\n",
    "# 1. Create the vertices DataFrame: union of unique IDs from col_a_3 and col_a_7.\n",
    "vertices = (\n",
    "    df.select(F.col(\"col_a_3\").alias(\"id\"))\n",
    "      .union(df.select(F.col(\"col_a_7\").alias(\"id\")))\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# 2. Create the edges DataFrame: define edge from col_a_3 to col_a_7.\n",
    "edges = df.select(F.col(\"col_a_3\").alias(\"src\"), F.col(\"col_a_7\").alias(\"dst\"))\n",
    "\n",
    "# 3. Compute out-degrees: count of outgoing edges for each source vertex.\n",
    "out_degrees = edges.groupBy(\"src\").agg(F.count(\"*\").alias(\"out_degree\"))\n",
    "\n",
    "# 4. Initialize each vertex with a PageRank value:\n",
    "#    The seed gets 1.0 and all others start with 0.0.\n",
    "vertices_rank = vertices.withColumn(\n",
    "    \"rank\", F.when(F.col(\"id\") == seed, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# 5. Iteratively update the PageRank values.\n",
    "for i in range(maxIter):\n",
    "    # 5a. For each edge, compute the contribution from its source.\n",
    "    #     Join the edges with the current vertex ranks and the out-degrees.\n",
    "    contribs = (\n",
    "        edges.join(vertices_rank, edges.src == vertices_rank.id)\n",
    "             .join(out_degrees, edges.src == out_degrees.src)\n",
    "             .select(\n",
    "                 edges.dst.alias(\"id\"),\n",
    "                 (vertices_rank.rank / out_degrees.out_degree).alias(\"contrib\")\n",
    "             )\n",
    "    )\n",
    "    \n",
    "    # 5b. Sum the contributions arriving at each vertex.\n",
    "    contribs_sum = contribs.groupBy(\"id\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "    \n",
    "    # 5c. Compute the total rank from dangling nodes (vertices with no outgoing edges).\n",
    "    dangling = (\n",
    "        vertices_rank.join(out_degrees, vertices_rank.id == out_degrees.src, \"left\")\n",
    "                     .withColumn(\"out_degree\", F.coalesce(F.col(\"out_degree\"), F.lit(0)))\n",
    "                     .where(F.col(\"out_degree\") == 0)\n",
    "    )\n",
    "    dangling_sum = dangling.agg(F.sum(\"rank\").alias(\"dangling_sum\")).collect()[0][\"dangling_sum\"]\n",
    "    if dangling_sum is None:\n",
    "        dangling_sum = 0.0\n",
    "    \n",
    "    # 5d. Update each vertex's rank:\n",
    "    #     - If the vertex is the seed, it gets the reset term (alpha)\n",
    "    #       plus (1 - alpha) times (its incoming contributions plus dangling rank).\n",
    "    #     - Otherwise, it just gets (1 - alpha) times its incoming contributions.\n",
    "    vertices_rank = (\n",
    "        vertices.join(contribs_sum, on=\"id\", how=\"left\")\n",
    "                .na.fill({\"sum_contrib\": 0.0})\n",
    "                .withColumn(\"rank\", \n",
    "                    F.when(F.col(\"id\") == seed,\n",
    "                           alpha + (1 - alpha) * (F.col(\"sum_contrib\") + dangling_sum)\n",
    "                    ).otherwise(\n",
    "                           (1 - alpha) * F.col(\"sum_contrib\")\n",
    "                    )\n",
    "                )\n",
    "    )\n",
    "\n",
    "# 6. Display the final personalized PageRank values.\n",
    "vertices_rank.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d45fbc-6044-484e-986c-4b693b830f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Barabasi-Albert Graph\n",
    "\n",
    "### Took 49 min to run on 2 Cores (DBC Edition generic nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3c6907-d879-4314-8dfb-ed305cfba507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark import RDD\n",
    "import random\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BA Graph Generator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def generate_five_tuple(ba_col_a1, ba_col_b1):\n",
    "    \"\"\"\n",
    "    Generate five-tuple data for an edge between nodes.\n",
    "    Returns a tuple\n",
    "    \"\"\"\n",
    "    ba_col_a2 = random.randint(1024, 65535)\n",
    "    ba_col_b2 = random.randint(1024, 65535)\n",
    "    ba_col_c = random.choice([\"A\", \"B\"])\n",
    "    return (ba_col_a1, ba_col_b1, ba_col_a2, ba_col_b2, ba_col_c)\n",
    "\n",
    "def generate_ba_graph_rdd(total_nodes, m):\n",
    "    \"\"\"\n",
    "    Generate the BA graph using RDDs for parallelism.\n",
    "    Each edge is generated in parallel across the cluster.\n",
    "\n",
    "    - total_nodes: Total number of nodes in the graph.\n",
    "    - m: Number of edges each new node will attach to.\n",
    "    \n",
    "    Returns an RDD of edges.\n",
    "    Each edge is represented as a tuple:\n",
    "      (ba_col_a1, ba_col_b1, ba_col_a2, ba_col_b2, ba_col_c)\n",
    "    \"\"\"\n",
    "    # Step 1: Create an initial complete graph among the first m nodes.\n",
    "    initial_nodes = list(range(m))\n",
    "    initial_edges = []\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(i + 1, m):\n",
    "            five_tuple = generate_five_tuple(i, j)\n",
    "            initial_edges.append((five_tuple[0], five_tuple[1], five_tuple[2], five_tuple[3], five_tuple[4]))\n",
    "\n",
    "    # Step 2: Create an RDD from initial edges\n",
    "    edges_rdd = spark.sparkContext.parallelize(initial_edges)\n",
    "\n",
    "    # Step 3: Maintain a list of nodes repeated by their degree (using RDD).\n",
    "    repeated_nodes = initial_nodes * (m - 1)\n",
    "    \n",
    "    # Broadcast the repeated nodes so each worker can access it\n",
    "    repeated_nodes_broadcast = spark.sparkContext.broadcast(repeated_nodes)\n",
    "\n",
    "    # Step 4: Generate new edges for each new node in parallel\n",
    "    def generate_edges_for_new_node(new_node):\n",
    "        targets = set()\n",
    "        # Sample m unique target nodes with probability proportional to degree.\n",
    "        while len(targets) < m:\n",
    "            target = random.choice(repeated_nodes_broadcast.value)  # Use the broadcasted value\n",
    "            targets.add(target)\n",
    "        new_edges = []\n",
    "        for target in targets:\n",
    "            five_tuple = generate_five_tuple(new_node, target)\n",
    "            new_edges.append((five_tuple[0], five_tuple[1], five_tuple[2], five_tuple[3], five_tuple[4]))\n",
    "        return new_edges\n",
    "\n",
    "    # Step 5: Generate edges for all new nodes from m to total_nodes\n",
    "    new_edges_rdd = spark.sparkContext.parallelize(range(m, total_nodes)) \\\n",
    "        .flatMap(generate_edges_for_new_node)\n",
    "\n",
    "    # Combine initial edges and newly generated edges\n",
    "    return edges_rdd.union(new_edges_rdd)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parameters for BA graph\n",
    "    total_nodes = 100_000_000   # Adjust as needed\n",
    "    m = 3                       # Each new node connects to m existing nodes\n",
    "\n",
    "    # Generate the BA graph edges as an RDD\n",
    "    edges_rdd = generate_ba_graph_rdd(total_nodes, m)\n",
    "\n",
    "    # Define a schema for the edges DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"ba_col_a1\", StringType(), False),\n",
    "        StructField(\"ba_col_b1\", StringType(), False),\n",
    "        StructField(\"ba_col_a2\", IntegerType(), False),\n",
    "        StructField(\"ba_col_b2\", IntegerType(), False),\n",
    "        StructField(\"ba_col_c\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # Convert the RDD to a DataFrame\n",
    "    edges_df = spark.createDataFrame(edges_rdd, schema)\n",
    "    edges_df.cache()\n",
    "\n",
    "    # Show a few rows of the generated graph\n",
    "    edges_df.show(10, truncate=False)\n",
    "\n",
    "    # Optionally, write the graph data to disk (e.g., CSV)\n",
    "    # edges_df.write.csv(\"ba_graph_output.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97898f96-fb3d-46e1-8c36-83c0f1410dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1564833091230482,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) HPE_NVDA_TEST",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
