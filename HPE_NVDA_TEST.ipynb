{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c271641c-fb06-4735-a64d-de5a4670d1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, BooleanType, DateType, TimestampType, ArrayType\n",
    ")\n",
    "# Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ExcelMetadataWorkflow\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb5f91a-4074-458b-8227-b2db8a003e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\r\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\n\u001B[?25l\r\u001B[K     |█▎                              | 10 kB 16.7 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 20 kB 6.5 MB/s eta 0:00:01\r\u001B[K     |████                            | 30 kB 9.1 MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 40 kB 3.0 MB/s eta 0:00:01\r\u001B[K     |██████▌                         | 51 kB 3.6 MB/s eta 0:00:01\r\u001B[K     |███████▉                        | 61 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |█████████▏                      | 71 kB 5.0 MB/s eta 0:00:01\r\u001B[K     |██████████▌                     | 81 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |███████████▊                    | 92 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 102 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████████████▍                 | 112 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |███████████████▊                | 122 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 133 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████▎             | 143 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████▋            | 153 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 163 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 174 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▌        | 184 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▉       | 194 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▏     | 204 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▍    | 215 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▊   | 225 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████  | 235 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▍| 245 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 250 kB 4.3 MB/s \r\n\u001B[?25hCollecting et-xmlfile\r\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\r\nInstalling collected packages: et-xmlfile, openpyxl\r\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-6f923ae3-0a6e-47b9-977a-636418ce1b63/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171fc0c9-c0b0-4609-875e-edd205561ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sheets: ['data_sources', 'joins', 'all_joins', 'table_a', 'table_b', 'table_c', 'table_d', 'table_e']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 1. Read all sheets from the Excel file.\n",
    "# -------------------------------------\n",
    "excel_path = \"/tmp/HPE_NVDA_datagen.xlsx\"  # update this path\n",
    "\n",
    "# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "# sheets = spark.read.  \n",
    "sheet_names = list(sheets.keys())\n",
    "print(\"Found sheets:\", sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834dbb3a-fefd-4d82-941b-0f3afb06dc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables and approximate row counts:\n  table_a: ~100000000000 rows\n  table_b: ~14000 rows\n  table_c: ~2100000000 rows\n  table_d: ~12000000 rows\n  table_e: ~33000000 rows\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 2. Process the tables overview (first sheet)\n",
    "# -------------------------------------\n",
    "# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\n",
    "tables_overview_df = sheets[sheet_names[0]]\n",
    "# Adjust these column names if your Excel file uses different names.\n",
    "table_names = tables_overview_df[\"masked_table_id\"].tolist()\n",
    "approx_row_counts = tables_overview_df[\"num_rows_approx\"].tolist()\n",
    "\n",
    "print(\"Tables and approximate row counts:\")\n",
    "for tbl, cnt in zip(table_names, approx_row_counts):\n",
    "    print(f\"  {tbl}: ~{cnt} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d419e44-8ad0-4faf-9792-3e5111c3421c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for table 'table_a'.\nLoaded metadata for table 'table_b'.\nLoaded metadata for table 'table_c'.\nLoaded metadata for table 'table_d'.\nLoaded metadata for table 'table_e'.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 3. Read each table's metadata (columns, types, etc.)\n",
    "# -------------------------------------\n",
    "# Here we assume that the sheet name for each table is the same as the table name.\n",
    "table_metadata = {}\n",
    "for tbl in table_names:\n",
    "    if tbl in sheets:\n",
    "        meta_df = sheets[tbl]\n",
    "        table_metadata[tbl] = meta_df\n",
    "        print(f\"Loaded metadata for table '{tbl}'.\")\n",
    "    else:\n",
    "        print(f\"Warning: No metadata sheet found for table '{tbl}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0a6c33-84f3-4c34-8366-8e1f79d59343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for table 'table_a': StructType([StructField('col_a_0', StringType(), True), StructField('col_a_1', LongType(), True), StructField('col_a_2', LongType(), True), StructField('col_a_3', IntegerType(), True), StructField('col_a_4', ArrayType(IntegerType(), True), True), StructField('col_a_5', LongType(), True), StructField('col_a_6', LongType(), True), StructField('col_a_7', IntegerType(), True), StructField('col_a_8', ArrayType(IntegerType(), True), True), StructField('col_a_9', IntegerType(), True), StructField('col_a_10', LongType(), True), StructField('col_a_11', LongType(), True), StructField('col_a_12', LongType(), True), StructField('col_a_13', LongType(), True)])\nSchema for table 'table_b': StructType([StructField('col_b_0', StringType(), True), StructField('col_b_1', IntegerType(), True), StructField('col_b_2', StringType(), True), StructField('col_b_3', IntegerType(), True), StructField('col_b_4', LongType(), True), StructField('col_b_5', StringType(), True), StructField('col_b_6', StringType(), True), StructField('col_b_7', StringType(), True), StructField('col_b_8', LongType(), True), StructField('col_b_9', LongType(), True), StructField('col_b_10', IntegerType(), True), StructField('col_b_11', LongType(), True), StructField('col_b_12', LongType(), True), StructField('col_b_13', StringType(), True)])\nSchema for table 'table_c': StructType([StructField('col_c_0', StringType(), True), StructField('col_c_1', ArrayType(IntegerType(), True), True), StructField('col_c_2', StringType(), True), StructField('col_c_3', ArrayType(IntegerType(), True), True), StructField('col_c_4', LongType(), True), StructField('col_c_5', LongType(), True), StructField('col_c_6', StringType(), True), StructField('col_c_7', LongType(), True), StructField('col_c_8', LongType(), True), StructField('col_c_9', LongType(), True), StructField('col_c_10', LongType(), True), StructField('col_c_11', IntegerType(), True), StructField('col_c_12', StringType(), True)])\nSchema for table 'table_d': StructType([StructField('col_d_0', LongType(), True), StructField('col_d_1', LongType(), True), StructField('col_d_2', StringType(), True), StructField('col_d_3', StringType(), True), StructField('col_d_4', LongType(), True), StructField('col_d_5', StringType(), True), StructField('col_d_6', StringType(), True), StructField('col_d_7', StringType(), True)])\nSchema for table 'table_e': StructType([StructField('col_e_0', LongType(), True), StructField('col_e_1', StringType(), True), StructField('col_e_2', StringType(), True), StructField('col_e_3', LongType(), True), StructField('col_e_4', StringType(), True), StructField('col_e_5', ArrayType(StringType(), True), True), StructField('col_e_6', ArrayType(StringType(), True), True), StructField('col_e_7', ArrayType(StringType(), True), True), StructField('col_e_8', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 4. Define a mapping from your Excel type names to Spark types.\n",
    "# -------------------------------------\n",
    "spark_type_mapping = {\n",
    "    \"StringType()\": StringType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"LongType()\": LongType(),\n",
    "    \"FloatType()\": FloatType(),\n",
    "    \"DoubleType()\": DoubleType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"DateType()\": DateType(),\n",
    "    \"TimestampType()\": TimestampType(),\n",
    "    \"ArrayType(IntegerType(), True)\": ArrayType(IntegerType(), True),\n",
    "    \"ArrayType(StringType(), True)\": ArrayType(StringType(), True)\n",
    "}\n",
    "\n",
    "def create_schema(meta_df):\n",
    "    \"\"\"\n",
    "    Create a Spark schema (StructType) from the metadata DataFrame.\n",
    "    For numerical types, if \"min\" and \"max\" are provided, they are stored in the field metadata.\n",
    "    This version ensures that the type from the spreadsheet is used (if it matches).\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    # Ensure that the range columns exist in the DataFrame.\n",
    "    has_range = (\"min\" in meta_df.columns) and (\"max\" in meta_df.columns)\n",
    "    \n",
    "    for idx, row in meta_df.iterrows():\n",
    "        col_name = row[\"masked_column_name\"]\n",
    "        # Convert the Type from the spreadsheet to a lower-case string.\n",
    "        type_str = str(row[\"spark_data_type\"]).strip() if pd.notna(row[\"spark_data_type\"]) else \"string\"\n",
    "        spark_type = spark_type_mapping.get(type_str)\n",
    "        \n",
    "        if spark_type is None:\n",
    "            # If the type is not recognized, warn and default to StringType.\n",
    "            print(f\"Warning: Unrecognized type '{row['spark_data_type']}' for column '{col_name}'. Using StringType.\")\n",
    "            spark_type = StringType()\n",
    "        \n",
    "        md = {}\n",
    "        # For numerical types, if min and max values are provided, store them in metadata.\n",
    "        if isinstance(spark_type, (IntegerType, LongType, FloatType, DoubleType)) and has_range:\n",
    "            if pd.notna(row[\"min\"]) and pd.notna(row[\"max\"]):\n",
    "                md[\"min\"] = row[\"min\"]\n",
    "                md[\"max\"] = row[\"max\"]\n",
    "        \n",
    "        fields.append(StructField(col_name, spark_type, True, metadata=md))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "# Create a dictionary of schemas for each table.\n",
    "schemas = {}\n",
    "for tbl, meta_df in table_metadata.items():\n",
    "    schema = create_schema(meta_df)\n",
    "    schemas[tbl] = schema\n",
    "    print(f\"Schema for table '{tbl}': {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a75abc-0d94-4c68-af52-03eefee5d730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join definitions:\n  table_a.col_a_1 LEFT JOIN table_d.col_d_0\n  table_a.col_a_1 LEFT JOIN table_e.col_e_0\n  table_a.col_a_5 LEFT JOIN table_e.col_e_0\n  table_c.col_c_9 LEFT JOIN table_e.col_e_0\n  table_c.col_c_10 LEFT JOIN table_e.col_e_0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 5. Process join information.\n",
    "# -------------------------------------\n",
    "# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\n",
    "join_info_df = sheets[sheet_names[1]]\n",
    "joins = []\n",
    "# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\n",
    "for idx, row in join_info_df.iterrows():\n",
    "    join_detail = {\n",
    "        \"left_table\": row[\"table1\"],\n",
    "        \"right_table\": row[\"table2\"],\n",
    "        \"join_method\": row[\"join_method\"],\n",
    "        \"left_column\": row[\"column1\"],\n",
    "        \"right_column\": row[\"column2\"]\n",
    "    }\n",
    "    joins.append(join_detail)\n",
    "\n",
    "print(\"Join definitions:\")\n",
    "for join in joins:\n",
    "    print(f\"  {join['left_table']}.{join['left_column']} {join['join_method'].upper()} JOIN {join['right_table']}.{join['right_column']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e54280-2160-4d6d-97aa-88d2096db7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed join query:\nSELECT * FROM table_a LEFT JOIN table_d ON table_a.col_a_1 = table_d.col_d_0 LEFT JOIN table_e ON table_a.col_a_1 = table_e.col_e_0 LEFT JOIN table_e ON table_a.col_a_5 = table_e.col_e_0 LEFT JOIN table_e ON table_c.col_c_9 = table_e.col_e_0 LEFT JOIN table_e ON table_c.col_c_10 = table_e.col_e_0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# (Optional) Step 6. Build a dynamic join query.\n",
    "# -------------------------------------\n",
    "# If you later load your data into Spark DataFrames and register them as temporary views,\n",
    "# you could build and execute a join query dynamically. For example, suppose you have:\n",
    "#    df_customers = spark.read.csv(\"customers.csv\", schema=schemas[\"customers\"], header=True)\n",
    "#    df_customers.createOrReplaceTempView(\"customers\")\n",
    "#    ... and similarly for other tables.\n",
    "#\n",
    "# The code below builds a join SQL string assuming sequential joining.\n",
    "if table_names:\n",
    "    join_query = f\"SELECT * FROM {table_names[0]}\"\n",
    "    for join in joins:\n",
    "        # Note: This simple logic assumes that the join order is appropriate.\n",
    "        join_query += (\n",
    "            f\" {join['join_method'].upper()} JOIN {join['right_table']} \"\n",
    "            f\"ON {join['left_table']}.{join['left_column']} = {join['right_table']}.{join['right_column']}\"\n",
    "        )\n",
    "    print(\"Constructed join query:\")\n",
    "    print(join_query)\n",
    "    # To execute the query once the tables are registered as temp views:\n",
    "    # result_df = spark.sql(join_query)\n",
    "    # result_df.show()\n",
    "\n",
    "# -------------------------------------\n",
    "# Now you have:\n",
    "#  - 'table_names' and 'approx_row_counts' from the overview.\n",
    "#  - 'table_metadata': a dictionary mapping table names to their metadata DataFrames.\n",
    "#  - 'schemas': a dictionary mapping table names to Spark schemas.\n",
    "#  - 'joins': a list of dictionaries describing the join relationships.\n",
    "#\n",
    "# You can now use this information to drive your Spark ETL/processing workflow.\n",
    "#\n",
    "# When finished, stop the Spark session (if running in a script).\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a369a5-f517-4f7f-b051-45b9fcef8119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Actually Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbb405f-fd8b-42d3-ab31-f479100d0ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame for table 'table_a' with 100000000 random rows.\nCreated DataFrame for table 'table_b' with 14000 random rows.\nCreated DataFrame for table 'table_c' with 21000000 random rows.\nCreated DataFrame for table 'table_d' with 12000000 random rows.\nCreated DataFrame for table 'table_e' with 33000000 random rows.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# PART 2: Generate random data for each table and register as temp views\n",
    "# ========================================\n",
    "\n",
    "def generate_random_dataframe(schema, num_rows):\n",
    "    \"\"\"\n",
    "    Given a Spark StructType schema and a number of rows, generate a DataFrame with random data\n",
    "    using Spark’s distributed operations.\n",
    "    For numerical types, if metadata has \"min\" and \"max\", those bounds are used.\n",
    "    \"\"\"\n",
    "    # Start with a DataFrame with a column \"id\" (this DataFrame is generated in a distributed fashion)\n",
    "    df = spark.range(num_rows)\n",
    "    \n",
    "    # For each field in the schema, add a column with a random value.\n",
    "    for field in schema.fields:\n",
    "        col_name = field.name\n",
    "        dt = field.dataType\n",
    "        md = field.metadata or {}\n",
    "        \n",
    "        if isinstance(dt, (IntegerType, LongType)):\n",
    "            # Use provided min and max if available; otherwise default to 1 and 1000.\n",
    "            min_val = md.get(\"min\", 1)\n",
    "            max_val = md.get(\"max\", 1000)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            # Cast appropriately.\n",
    "            if isinstance(dt, IntegerType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"int\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"long\"))\n",
    "                \n",
    "        elif isinstance(dt, (FloatType, DoubleType)):\n",
    "            min_val = md.get(\"min\", 0.0)\n",
    "            max_val = md.get(\"max\", 1000.0)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            if isinstance(dt, FloatType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"float\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"double\"))\n",
    "                \n",
    "        elif isinstance(dt, BooleanType):\n",
    "            # Generate a boolean value based on a threshold.\n",
    "            df = df.withColumn(col_name, F.rand() > 0.5)\n",
    "            \n",
    "        elif isinstance(dt, DateType):\n",
    "            # Generate a random date by adding a random number of days (e.g., 0 to 9000) to a base date.\n",
    "            df = df.withColumn(col_name, F.expr(\"date_add('2000-01-01', cast(rand() * 9000 as int))\"))\n",
    "            \n",
    "        elif isinstance(dt, TimestampType):\n",
    "            # Generate a random timestamp by first generating a random date and then converting it.\n",
    "            df = df.withColumn(col_name, F.expr(\"to_timestamp(date_add('2000-01-01', cast(rand() * 9000 as int)))\"))\n",
    "            \n",
    "        elif isinstance(dt, StringType):\n",
    "            # Use the built-in uuid() function for random strings.\n",
    "            df = df.withColumn(col_name, F.expr(\"uuid()\"))\n",
    "            \n",
    "        else:\n",
    "            # For any unrecognized type, set the column to null.\n",
    "            df = df.withColumn(col_name, F.lit(None))\n",
    "            \n",
    "    # Drop the original \"id\" column.\n",
    "    return df.drop(\"id\")\n",
    "\n",
    "# Create and register a DataFrame for each table using the distributed random data generation.\n",
    "# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\n",
    "dfs = {}\n",
    "for tbl, count in zip(table_names, approx_row_counts):\n",
    "    schema = schemas[tbl]\n",
    "    if tbl == 'table_a':\n",
    "        num_rows = 100000000\n",
    "    elif tbl == 'table_c':\n",
    "        num_rows = 21000000\n",
    "    else:\n",
    "        num_rows = int(count)\n",
    "    # num_rows = int(count)\n",
    "    df = generate_random_dataframe(schema, num_rows)\n",
    "    dfs[tbl] = df\n",
    "    print(f\"Created DataFrame for table '{tbl}' with {num_rows} random rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1945eb13-1f2f-4de6-ae17-86d261932513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GroupBys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f53375a-a78f-4275-8651-3f74442844a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test1 = table_a.groupBy(['col_a_1', 'col_a_3', 'col_a_5', 'col_a_7', 'col_a_9']).count()\n",
    "\n",
    "gb_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d734fac-d3b0-4cf3-bc2f-e3e837cc89e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "gb_test2 = table_a.groupBy(['col_a_1']).count()\n",
    "\n",
    "gb_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b983468b-a711-4502-ade9-544f2f24e894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7aae09-ab6a-427c-bbd5-4abe614648a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### table_c `col_1` and `col_3` are both ArrayType(IntegerType) columns in real life, most of the time the array is one value long, but sometimes it's multiple values. Try and figure this out at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb51607-cddb-48be-a4a9-de6da2816eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_b = dfs['table_b']\n",
    "\n",
    "join_test1 = table_a.join(table_b, [\n",
    "    table_a[\"col_a_1\"]==table_b[\"col_b_8\"],\n",
    "    table_a[\"col_a_3\"]==table_b[\"col_b_3\"],\n",
    "    table_a[\"col_a_5\"]==table_b[\"col_b_9\"],\n",
    "    table_a[\"col_a_7\"]==table_b[\"col_b_1\"],\n",
    "],\n",
    "how='left')\n",
    "join_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba0b929-c3d6-43f6-a18a-31039e496c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_c = dfs['table_c']\n",
    "\n",
    "join_test2 = table_a.join(table_c, [\n",
    "    table_a[\"col_a_1\"]==table_c[\"col_c_10\"],\n",
    "    table_a[\"col_a_3\"]==table_c[\"col_c_9\"],\n",
    "    table_a[\"col_a_9\"]==table_c[\"col_c_11\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e02e0c4-5c04-467e-b533-b345b308a18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_d = dfs['table_d']\n",
    "\n",
    "join_test3 = table_a.join(table_d, [\n",
    "    table_a[\"col_a_1\"]==table_d[\"col_d_0\"],\n",
    "    table_a[\"col_a_5\"]==table_d[\"col_d_1\"],\n",
    "],\n",
    "how='left')\n",
    "\n",
    "join_test3.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5824fe-5fae-4f63-8dd9-117b185489e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_a = dfs['table_a']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test4 = table_a.join(table_e, table_a[\"col_a_1\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test4.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0635d3-d4ba-4760-aa23-38fcca5730dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test5 = table_c.join(table_e, table_c[\"col_c_5\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test5.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085d4042-5da3-43ec-b0f9-db94acec7e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_c = dfs['table_c']\n",
    "table_e = dfs['table_e']\n",
    "\n",
    "join_test6 = table_c.join(table_e, table_c[\"col_c_10\"]==table_e[\"col_e_0\"], how='left')\n",
    "\n",
    "join_test6.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17eab97-b90d-4f91-b6c3-fc95b07b38e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "linked_join_test = (\n",
    "    table_a\n",
    "    .join(\n",
    "        table_b,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_b[\"col_b_8\"],\n",
    "            table_a[\"col_a_3\"] == table_b[\"col_b_3\"],\n",
    "            table_a[\"col_a_5\"] == table_b[\"col_b_9\"],\n",
    "            table_a[\"col_a_7\"] == table_b[\"col_b_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_c,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_c[\"col_c_10\"],\n",
    "            table_a[\"col_a_3\"] == table_c[\"col_c_9\"],\n",
    "            table_a[\"col_a_9\"] == table_c[\"col_c_11\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_d,\n",
    "        [\n",
    "            table_a[\"col_a_1\"] == table_d[\"col_d_0\"],\n",
    "            table_a[\"col_a_5\"] == table_d[\"col_d_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_e,\n",
    "        table_a[\"col_a_1\"] == table_e[\"col_e_0\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "linked_join_test.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d3c6907-d879-4314-8dfb-ed305cfba507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1564833091230482,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) HPE_NVDA_TEST",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
