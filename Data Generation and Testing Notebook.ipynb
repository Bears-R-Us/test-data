{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2639b9-a610-46a8-824a-e59036ea7022",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79a761-e953-4578-b095-c29ba57dad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab2a7d-e9a0-4100-a248-69d368abefef",
   "metadata": {},
   "source": [
    "### Start Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934fd04-4acd-4d1c-be90-b24a612cb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, BooleanType, DateType, TimestampType, ArrayType\n",
    ")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"spark://masternode\") \\ # MUST BE CHANGED TO YOUR MASTER NODE\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.executor.cores\",12) \\\n",
    "    .config(\"spark.executor.instances\",4) \\\n",
    "    .config(\"spark.executor.memory\",\"16G\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\",\"160mb\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\",\"32mb\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\",\"2gb\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\",200) \\\n",
    "    .config(\"spark.task.cpus\",1) \\\n",
    "    .config(\"spark.sql.legacy.charVarcharAsString\",True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c6c4b-c699-4958-be69-d2b3d3804658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_synthetic_distribution(params, plot=True):\n",
    "\n",
    "    slope = params.get('slope', -2)\n",
    "    min_degree = params.get('min_degree', 1)\n",
    "    max_degree = params.get('max_degree', 200_000)\n",
    "    max_prob = params.get('max_prob', 0.5)\n",
    "\n",
    "    # Create an array of degrees from min_degree to max_degree as floats\n",
    "    degrees = np.arange(min_degree, max_degree + 1, dtype=float)\n",
    "\n",
    "    # Calculate the scaling factor A to ensure the maximum probability at min_degree\n",
    "    A = max_prob / (min_degree ** slope)\n",
    "\n",
    "    # Compute the power-law decay values\n",
    "    y_values = A * degrees ** slope\n",
    "\n",
    "    # Convert degrees to integers for dictionary keys\n",
    "    degrees_int = degrees.astype(int)\n",
    "\n",
    "    # Create a dictionary mapping degrees to decay values\n",
    "    decay_dict = dict(zip(degrees_int, y_values))\n",
    "\n",
    "    return decay_dict\n",
    "\n",
    "params = {\n",
    "    'slope': -2,\n",
    "    'intercept': 0.8,\n",
    "    'r_squared': 0.98,\n",
    "    'max_degree': 200_000,\n",
    "    'min_degree': 1,\n",
    "    'max_prob': 0.5,\n",
    "    'degree_range': list(np.arange(1, 200_000))\n",
    "}\n",
    "\n",
    "target_distribution = create_synthetic_distribution(params, 200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a61f6e-1da4-421c-9a1a-b25184b76188",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = spark.sparkContext.defaultParallelism # number of cores available\n",
    "num_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd09b9-16fb-4c61-8cb5-084040f0e70e",
   "metadata": {},
   "source": [
    "### Create Graph Based on Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bcb81-94af-4fcb-b5f4-14d9c74bbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import random\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"NetworkFlowGraph\").getOrCreate()\n",
    "\n",
    "def random_node():\n",
    "    # return '.'.join(map(str, np.random.randint(0, 256, size=4).tolist()))\n",
    "    return int(np.random.randint(1_000_000, 10_000_000_000))\n",
    "\n",
    "def random_feature():\n",
    "    return int(np.random.randint(1, 70000))  # cast to native int\n",
    "\n",
    "def random_col_e():\n",
    "    return str(np.random.choice(['col_e_A', 'col_e_B']))  # cast to native str\n",
    "\n",
    "# num_graphs = spark.sparkContext.defaultParallelism # number of cores available\n",
    "num_nodes_per_graph = 350_000\n",
    "\n",
    "def configuration_model_with_distribution(n, degree_distribution,seed):\n",
    "    \"\"\"\n",
    "    Generate a graph with a specific degree distribution\n",
    "    \"\"\"\n",
    "\n",
    "    degrees = []\n",
    "    remaining_nodes = n\n",
    "\n",
    "    for degree, prob in sorted(degree_distribution.items()):\n",
    "        if remaining_nodes <= 0:\n",
    "            break\n",
    "        count = min(int(n * prob + 0.5), remaining_nodes)\n",
    "        if count > 0:\n",
    "            degrees.extend([int(degree)] * count)\n",
    "            remaining_nodes -= count\n",
    "\n",
    "    if remaining_nodes > 0:\n",
    "        min_degree = min(degree_distribution.keys())\n",
    "        degrees.extend([min_degree] * remaining_nodes)\n",
    "\n",
    "    if len(degrees) < 2:\n",
    "        degrees = [1, 1]\n",
    "\n",
    "    if sum(degrees) % 2 != 0:\n",
    "        degrees[0] += 1\n",
    "\n",
    "    try:\n",
    "        g = nx.configuration_model(degrees, seed=seed)\n",
    "        g = nx.Graph(g)\n",
    "\n",
    "        if g.number_of_edges() == 0:\n",
    "            raise nx.NetworkXError(\"Generated graph has no edges\")\n",
    "\n",
    "        return g\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating graph: {e}\")\n",
    "        return nx.barabasi_ablert_graph(n, 2)\n",
    "    \n",
    "def generate_custom_graph(partition_id, num_nodes, degree_distribution, seed):\n",
    "    # np.random.seed(seed + partition_id)\n",
    "    # random.seed(seed + partition_id)\n",
    "\n",
    "    g = configuration_model_with_distribution(num_nodes, degree_distribution, seed + partition_id)\n",
    "\n",
    "    node_map = {node: random_node() for node in g.nodes()}\n",
    "    edges = [(node_map[edge[0]], node_map[edge[1]],\n",
    "              random_feature(), random_feature(), random_col_e())\n",
    "             for edge in g.edges()]\n",
    "    \n",
    "    return edges\n",
    "\n",
    "target_distribution_bc = spark.sparkContext.broadcast(target_distribution)\n",
    "\n",
    "directory_path = \"/path/to/synthetic_dataset\"\n",
    "seed = 1\n",
    "remaining_iterations = 10\n",
    "\n",
    "# while get_directory_size(directory_path) < MAX_SIZE_BYTES:\n",
    "start = time.time()\n",
    "while remaining_iterations > 0:\n",
    "    print(remaining_iterations)\n",
    "    seed_bc = spark.sparkContext.broadcast(seed)\n",
    "    generate_custom_graph_udf = udf(\n",
    "        lambda partition_id: generate_custom_graph(\n",
    "            partition_id, num_nodes_per_graph, target_distribution_bc.value, seed_bc.value\n",
    "        ),\n",
    "        ArrayType(StructType([\n",
    "            StructField(\"col_a\", LongType(), False),\n",
    "            StructField(\"col_b\", LongType(), False),\n",
    "            StructField(\"col_c\", IntegerType(), False),\n",
    "            StructField(\"col_d\", IntegerType(), False),\n",
    "            StructField(\"col_e\", StringType(), False),\n",
    "        ]))\n",
    "    )\n",
    "\n",
    "    edge_df = (\n",
    "        spark.range(num_graphs)\n",
    "             .withColumn(\"edges\", generate_custom_graph_udf(\"id\"))\n",
    "             .select(explode(\"edges\").alias(\"edge\"))\n",
    "             .select(\"edge.col_a\", \"edge.col_b\", \"edge.col_c\", \"edge.col_d\", \"edge.col_e\")\n",
    "             .distinct()  # if needed\n",
    "    )\n",
    "\n",
    "    edge_df.write.mode(\"append\").parquet(directory_path)\n",
    "\n",
    "    seed += 1\n",
    "    remaining_iterations -= 1\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TTR 10 Iterations: {round(end-start, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467b604-4222-41e1-9279-12cc7d91481f",
   "metadata": {},
   "source": [
    "### Alternative Option for Scaling: Double Size of Dataset and Add Random Noise (maintains distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5b98b-e6a1-4dcc-b9f8-78c34a02a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, floor, rand, lit\n",
    "\n",
    "# Define the noise range\n",
    "NOISE_MIN = -5\n",
    "NOISE_MAX = 5\n",
    "\n",
    "long_cols = [\"col_a\", \"col_b\"]\n",
    "integer_cols = [\"col_c\", \"col_d\"]\n",
    "string_cols = [\"col_e\"]\n",
    "\n",
    "# Original DataFrame\n",
    "df_original = test\n",
    "\n",
    "# Create the noisy copy\n",
    "df_augmented = df_original.select(\n",
    "    *[\n",
    "        # Add integer noise to numeric columns\n",
    "        (col(c) + floor(rand() * (NOISE_MAX - NOISE_MIN + 1)) + NOISE_MIN).cast(\"long\").alias(c)\n",
    "        for c in long_cols\n",
    "    ] + [\n",
    "        # Add integer noise to numeric columns\n",
    "        (col(c) + floor(rand() * (NOISE_MAX - NOISE_MIN + 1)) + NOISE_MIN).cast(\"integer\").alias(c)\n",
    "        for c in integer_cols\n",
    "    ] + [\n",
    "        # Preserve string columns\n",
    "        col(c) for c in string_cols\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Optionally add source labels\n",
    "df_original = df_original.withColumn(\"source\", lit(\"original\"))\n",
    "df_augmented = df_augmented.withColumn(\"source\", lit(\"noisy\"))\n",
    "\n",
    "# Combine both\n",
    "df_doubled = df_original.unionByName(df_augmented)\n",
    "\n",
    "df_doubled.repartition(2000).write.parquet('/path/to/synthetic_dataset_doubled', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06576-9d36-4ca0-a047-1fd8b65b5b71",
   "metadata": {},
   "source": [
    "### Get Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abe78e-109e-4704-bdcb-43590b4fb314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import random\n",
    "\n",
    "edge_df = df_doubled\n",
    "\n",
    "def print_graph_stats(edge_df, display=False):\n",
    "\n",
    "    unique_nodes = edge_df.select(\"col_a\").union(edge_df.select(\"col_b\")).distinct()\n",
    "\n",
    "    total_edges = edge_df.count()\n",
    "\n",
    "    src_degrees = edge_df.groupBy(\"col_a\").agg({\"col_a\": \"count\"}) \\\n",
    "        .withColumnRenamed(\"count(col_a)\", \"in_degree\") \\\n",
    "        .withColumnRenamed(\"col_a\", \"merge_col\")\n",
    "\n",
    "    dst_degrees = edge_df.groupBy(\"col_b\").agg({\"col_b\": \"count\"}) \\\n",
    "        .withColumnRenamed(\"count(col_b)\", \"out_degree\") \\\n",
    "        .withColumnRenamed(\"col_b\", \"merge_col\")\n",
    "\n",
    "    total_degrees = src_degrees.join(dst_degrees, \"merge_col\", \"outer\") \\\n",
    "        .fillna(0) \\\n",
    "            .selectExpr(\n",
    "                \"merge_col\", \n",
    "                \"cast(out_degree as int) + cast(in_degree as int) as total_degree\"\n",
    "            )\n",
    "        \n",
    "    avg_degree = total_degrees.agg({\"total_degree\": \"avg\"}).collect()[0][0]\n",
    "    max_degree = total_degrees.agg({\"total_degree\": \"max\"}).collect()[0][0]\n",
    "\n",
    "    if display:\n",
    "        print(\"\\nGraph Statistics:\")\n",
    "        print(f\"Number of unique Nodes: {unique_nodes.count()}\")\n",
    "        print(f\"Number of edges: {total_edges}\")\n",
    "        print(f\"Average degree: {avg_degree:.2f}\")\n",
    "        print(f\"Maximum degree: {max_degree}\")\n",
    "\n",
    "    return total_degrees\n",
    "\n",
    "def check_degree_distribution(edge_df, plot=False):\n",
    "    total_degrees = print_graph_stats(edge_df)\n",
    "\n",
    "    degree_dist = total_degrees.groupBy(\"total_degree\").count() \\\n",
    "        .withColumnRenamed(\"count\", \"num_vertices\")\n",
    "\n",
    "    total_nodes = total_degrees.count()\n",
    "\n",
    "    # Collect results to driver for plotting\n",
    "    dist_rows = degree_dist.collect()\n",
    "    data = {\n",
    "        \"total_degree\": [row[\"total_degree\"] for row in dist_rows],\n",
    "        \"num_vertices\": [row[\"num_vertices\"] for row in dist_rows]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"empirical\"] = df[\"num_vertices\"] / total_nodes\n",
    "    df[\"target\"] = df[\"total_degree\"].map(target_distribution).fillna(1e-12)  # avoid log(0)\n",
    "    \n",
    "    # Sort by total_degree\n",
    "    df = df.sort_values(by=\"total_degree\")\n",
    "\n",
    "    print(\"\\nResulting degree distribution:\")\n",
    "    for row in df.itertuples():\n",
    "        print(f\"Degree {row.total_degree}: {row.empirical:.4f} (Target: {row.target:.4f})\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['total_degree'], df['empirical'], marker='o', label='Empirical Distribution')\n",
    "        plt.plot(df['total_degree'], df['target'], marker='x', linestyle='--', label='Target Distribution')\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Percentage of Nodes (log scale)')\n",
    "        plt.yscale('log')\n",
    "        plt.title('Degree Distribution: Empirical vs Target (Log Scale)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print_graph_stats(edge_df, display=True)\n",
    "check_degree_distribution(edge_df, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d9499-f211-44bd-a3db-aa0766f4963d",
   "metadata": {},
   "source": [
    "### Generate Non-Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766235d5-a10b-4d5d-bb79-590a6d69e134",
   "metadata": {},
   "source": [
    "#### Get Workflow Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06393611-1bd6-4f29-8581-5bb0f40e13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 1. Read all sheets from the Excel file.\n",
    "# -------------------------------------\n",
    "excel_path = \"HPE_NVDA_datagen.xlsx\" # update this path as necessary\n",
    "\n",
    "# Read every sheet into a dictionary: keys are sheet names, values are DataFrames.\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "# sheets = spark.read.  \n",
    "sheet_names = list(sheets.keys())\n",
    "print(\"Found sheets:\", sheet_names)\n",
    "\n",
    "# -------------------------------------\n",
    "# Step 2. Process the tables overview (first sheet)\n",
    "# -------------------------------------\n",
    "# Assumption: The first sheet (e.g. \"Tables\") lists the table names and approximate row counts.\n",
    "tables_overview_df = sheets[sheet_names[0]]\n",
    "# Adjust these column names if your Excel file uses different names.\n",
    "table_names = tables_overview_df[\"masked_table_id\"].tolist()\n",
    "approx_row_counts = tables_overview_df[\"num_rows_approx\"].tolist()\n",
    "\n",
    "print(\"Tables and approximate row counts:\")\n",
    "for tbl, cnt in zip(table_names, approx_row_counts):\n",
    "    print(f\"  {tbl}: ~{cnt} rows\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Step 3. Read each table's metadata (columns, types, etc.)\n",
    "# -------------------------------------\n",
    "# Here we assume that the sheet name for each table is the same as the table name.\n",
    "table_metadata = {}\n",
    "for tbl in table_names:\n",
    "    if tbl in sheets:\n",
    "        meta_df = sheets[tbl]\n",
    "        table_metadata[tbl] = meta_df\n",
    "        print(f\"Loaded metadata for table '{tbl}'.\")\n",
    "    else:\n",
    "        print(f\"Warning: No metadata sheet found for table '{tbl}'.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Step 4. Define a mapping from your Excel type names to Spark types.\n",
    "# -------------------------------------\n",
    "spark_type_mapping = {\n",
    "    \"StringType()\": StringType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"IntegerType()\": IntegerType(),\n",
    "    \"LongType()\": LongType(),\n",
    "    \"FloatType()\": FloatType(),\n",
    "    \"DoubleType()\": DoubleType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"BooleanType()\": BooleanType(),\n",
    "    \"DateType()\": DateType(),\n",
    "    \"TimestampType()\": TimestampType(),\n",
    "    \"ArrayType(IntegerType(), True)\": ArrayType(IntegerType(), True),\n",
    "    \"ArrayType(StringType(), True)\": ArrayType(StringType(), True)\n",
    "}\n",
    "\n",
    "def create_schema(meta_df):\n",
    "    \"\"\"\n",
    "    Create a Spark schema (StructType) from the metadata DataFrame.\n",
    "    For numerical types, if \"min\" and \"max\" are provided, they are stored in the field metadata.\n",
    "    This version ensures that the type from the spreadsheet is used (if it matches).\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    # Ensure that the range columns exist in the DataFrame.\n",
    "    has_range = (\"min\" in meta_df.columns) and (\"max\" in meta_df.columns)\n",
    "    \n",
    "    for idx, row in meta_df.iterrows():\n",
    "        col_name = row[\"masked_column_name\"]\n",
    "        # Convert the Type from the spreadsheet to a lower-case string.\n",
    "        type_str = str(row[\"spark_data_type\"]).strip() if pd.notna(row[\"spark_data_type\"]) else \"string\"\n",
    "        spark_type = spark_type_mapping.get(type_str)\n",
    "        \n",
    "        if spark_type is None:\n",
    "            # If the type is not recognized, warn and default to StringType.\n",
    "            print(f\"Warning: Unrecognized type '{row['spark_data_type']}' for column '{col_name}'. Using StringType.\")\n",
    "            spark_type = StringType()\n",
    "        \n",
    "        md = {}\n",
    "        # For numerical types, if min and max values are provided, store them in metadata.\n",
    "        if isinstance(spark_type, (IntegerType, LongType, FloatType, DoubleType)) and has_range:\n",
    "            if pd.notna(row[\"min\"]) and pd.notna(row[\"max\"]):\n",
    "                md[\"min\"] = row[\"min\"]\n",
    "                md[\"max\"] = row[\"max\"]\n",
    "        \n",
    "        fields.append(StructField(col_name, spark_type, True, metadata=md))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "# Create a dictionary of schemas for each table.\n",
    "schemas = {}\n",
    "for tbl, meta_df in table_metadata.items():\n",
    "    schema = create_schema(meta_df)\n",
    "    schemas[tbl] = schema\n",
    "    print(f\"Schema for table '{tbl}': {schema}\")\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# Step 5. Process join information.\n",
    "# -------------------------------------\n",
    "# Assumption: The final sheet (last sheet) is named \"Joins\" and holds the join definitions.\n",
    "join_info_df = sheets[sheet_names[1]]\n",
    "joins = []\n",
    "# Here we assume join_info_df has columns: \"LeftTable\", \"LeftColumn\", \"RightTable\", \"RightColumn\", and optionally \"JoinType\"\n",
    "for idx, row in join_info_df.iterrows():\n",
    "    join_detail = {\n",
    "        \"left_table\": row[\"table1\"],\n",
    "        \"right_table\": row[\"table2\"],\n",
    "        \"join_method\": row[\"join_method\"],\n",
    "        \"left_column\": row[\"column1\"],\n",
    "        \"right_column\": row[\"column2\"]\n",
    "    }\n",
    "    joins.append(join_detail)\n",
    "\n",
    "print(\"Join definitions:\")\n",
    "for join in joins:\n",
    "    print(f\"  {join['left_table']}.{join['left_column']} {join['join_method'].upper()} JOIN {join['right_table']}.{join['right_column']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a1d87-6c3e-4afa-ad0a-229b8ba3f1a1",
   "metadata": {},
   "source": [
    "#### Generate Non-Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94c54b-a996-4e72-9168-5a2f1e63422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# PART 2: Generate random data for each table and register as temp views\n",
    "# ========================================\n",
    "\n",
    "def generate_random_dataframe(schema, num_rows):\n",
    "    \"\"\"\n",
    "    Given a Spark StructType schema and a number of rows, generate a DataFrame with random data\n",
    "    using Spark’s distributed operations.\n",
    "    For numerical types, if metadata has \"min\" and \"max\", those bounds are used.\n",
    "    \"\"\"\n",
    "    # Start with a DataFrame with a column \"id\" (this DataFrame is generated in a distributed fashion)\n",
    "    df = spark.range(num_rows)\n",
    "    \n",
    "    # For each field in the schema, add a column with a random value.\n",
    "    for field in schema.fields:\n",
    "        col_name = field.name\n",
    "        dt = field.dataType\n",
    "        md = field.metadata or {}\n",
    "        \n",
    "        if isinstance(dt, (IntegerType, LongType)):\n",
    "            # Use provided min and max if available; otherwise default to 1 and 1000.\n",
    "            min_val = md.get(\"min\", 1)\n",
    "            max_val = md.get(\"max\", 1000)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            # Cast appropriately.\n",
    "            if isinstance(dt, IntegerType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"int\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"long\"))\n",
    "                \n",
    "        elif isinstance(dt, (FloatType, DoubleType)):\n",
    "            min_val = md.get(\"min\", 0.0)\n",
    "            max_val = md.get(\"max\", 1000.0)\n",
    "            expr = (F.rand() * (float(max_val) - float(min_val)) + float(min_val))\n",
    "            if isinstance(dt, FloatType):\n",
    "                df = df.withColumn(col_name, expr.cast(\"float\"))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, expr.cast(\"double\"))\n",
    "                \n",
    "        elif isinstance(dt, BooleanType):\n",
    "            # Generate a boolean value based on a threshold.\n",
    "            df = df.withColumn(col_name, F.rand() > 0.5)\n",
    "            \n",
    "        elif isinstance(dt, DateType):\n",
    "            # Generate a random date by adding a random number of days (e.g., 0 to 9000) to a base date.\n",
    "            df = df.withColumn(col_name, F.expr(\"date_add('2000-01-01', cast(rand() * 9000 as int))\"))\n",
    "            \n",
    "        elif isinstance(dt, TimestampType):\n",
    "            # Generate a random timestamp by first generating a random date and then converting it.\n",
    "            df = df.withColumn(col_name, F.expr(\"to_timestamp(date_add('2000-01-01', cast(rand() * 9000 as int)))\"))\n",
    "            \n",
    "        elif isinstance(dt, StringType):\n",
    "            # Use the built-in uuid() function for random strings.\n",
    "            df = df.withColumn(col_name, F.expr(\"uuid()\"))\n",
    "            \n",
    "        else:\n",
    "            # For any unrecognized type, set the column to null.\n",
    "            df = df.withColumn(col_name, F.lit(None))\n",
    "            \n",
    "    # Drop the original \"id\" column.\n",
    "    return df.drop(\"id\")\n",
    "\n",
    "# Create and register a DataFrame for each table using the distributed random data generation.\n",
    "# NOTE: THIS WAS SCALED DOWN FOR TESTING PURPOSES. UNCOMMENT LINE 74 AND COMMENT OUT LINES 68-73 FOR REAL TESTING\n",
    "dfs = {}\n",
    "for tbl, count in zip(table_names, approx_row_counts):\n",
    "    if tbl != 'table_a':\n",
    "        schema = schemas[tbl]\n",
    "        if tbl == 'table_c':\n",
    "            num_rows = 21000000\n",
    "        else:\n",
    "            num_rows = int(count)\n",
    "        # num_rows = int(count)\n",
    "        df = generate_random_dataframe(schema, num_rows)\n",
    "        dfs[tbl] = df\n",
    "        print(f\"Created DataFrame for table '{tbl}' with {num_rows} random rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed2fbe-8584-42cb-9003-f4b45a6d7378",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18904868-f3a5-4c54-b064-c0aee6c4ba28",
   "metadata": {},
   "source": [
    "## Basic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b79c60-4c3c-490c-8297-ab2bcfc7a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = spark.read.parquet('/path/to/synthetic_dataset_doubled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6766270-3726-46b9-ac34-4dfb8ccec20f",
   "metadata": {},
   "source": [
    "### Sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76afa5-cfc7-4607-b0e7-5e6f37d438ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort1 = edge_df.orderBy(['col_a'])\n",
    "sort1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a83303-65d8-4bd2-8e8c-5fb4ce0dec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort2 = edge_df.orderBy(['col_a', 'col_b', 'col_c', 'col_d', 'col_e'])\n",
    "sort2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410be1b6-ebc9-4018-9624-20426079a9a9",
   "metadata": {},
   "source": [
    "### GroupBys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8cecc-3c46-410c-8dac-22afb4bfc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = edge_df.groupBy(['col_a']).count()\n",
    "gb1.write.format(\"noop\").mode(\"overwrite\").save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff77ec9-a2fd-451c-929e-b10255a2fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2 = edge_df.groupBy(['col_a', 'col_b', 'col_c', 'col_d', 'col_e']).count()\n",
    "gb2.write.format(\"noop\").mode(\"overwrite\").save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb9cf6-ad0a-41f7-825c-3d83f26e4f1c",
   "metadata": {},
   "source": [
    "## Workflow Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8ac7e-f351-4a4d-8031-047848444473",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0211dc-91b0-40cd-a66e-7266ff7c5867",
   "metadata": {},
   "source": [
    "#### Single Step Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b89f8-612a-4d2c-b1ed-24d4b23bbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_a = edge_df\n",
    "table_b = dfs['table_b']\n",
    "\n",
    "join_test1 = table_a.join(table_b, [\n",
    "        table_a[\"col_a\"]==table_b[\"col_b_8\"],\n",
    "        table_a[\"col_b\"]==table_b[\"col_b_3\"],\n",
    "        table_a[\"col_c\"]==table_b[\"col_b_9\"],\n",
    "        table_a[\"col_d\"]==table_b[\"col_b_1\"],\n",
    "    ],\n",
    "    how='left'\n",
    ")\n",
    "join_test1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3024ff-cac0-4002-9c3b-f5dd993ccfd5",
   "metadata": {},
   "source": [
    "#### Workflow Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44195c1-8940-479d-bcd7-8c65d3b7ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_join_test = (\n",
    "    table_a\n",
    "    .join(\n",
    "        table_b,\n",
    "        [\n",
    "            table_a[\"col_a\"] == table_b[\"col_b_8\"],\n",
    "            table_a[\"col_b\"] == table_b[\"col_b_3\"],\n",
    "            table_a[\"col_c\"] == table_b[\"col_b_9\"],\n",
    "            table_a[\"col_d\"] == table_b[\"col_b_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_c,\n",
    "        [\n",
    "            table_a[\"col_a\"] == table_c[\"col_c_10\"],\n",
    "            table_a[\"col_b\"] == table_c[\"col_c_9\"],\n",
    "            table_a[\"col_e\"] == table_c[\"col_c_11\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_d,\n",
    "        [\n",
    "            table_a[\"col_a\"] == table_d[\"col_d_0\"],\n",
    "            table_a[\"col_c\"] == table_d[\"col_d_1\"],\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        table_e,\n",
    "        table_a[\"col_a\"] == table_e[\"col_e_0\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "linked_join_test.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41835272-d89f-41e8-9c55-c92a94e080f1",
   "metadata": {},
   "source": [
    "### Breadth First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981fc98-fec5-4115-b3b0-ab6c3a7231f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assume your input DataFrame 'df' has columns \"col_0\" and \"col_1\".\n",
    "# We create an 'edges' DataFrame with \"src\" and \"dst\" columns.\n",
    "df = edge_df\n",
    "\n",
    "edges = df.select(F.col(\"col_b\").alias(\"src\"), F.col(\"col_d\").alias(\"dst\"))\n",
    "\n",
    "# Define the BFS starting point.\n",
    "# Change the 'source' variable to the vertex from which you want to start the BFS.\n",
    "source = 1000  # For example, use \"A\" as the starting vertex\n",
    "\n",
    "# Create the initial frontier: the source vertex with distance 0.\n",
    "frontier = spark.createDataFrame([(source, 0)], [\"vertex\", \"distance\"])\n",
    "\n",
    "# Create a DataFrame to keep track of all visited vertices (and their distance from the source).\n",
    "visited = frontier\n",
    "\n",
    "# Loop until there are no new nodes to visit.\n",
    "while frontier.count() > 0:\n",
    "    # 1. Find neighbors: join the current frontier with the edges DataFrame.\n",
    "    #    Each neighbor gets a distance equal to (current distance + 1).\n",
    "    new_neighbors = frontier.join(edges, frontier.vertex == edges.src) \\\n",
    "                            .select(edges.dst.alias(\"vertex\"),\n",
    "                                    (frontier.distance + 1).alias(\"distance\"))\n",
    "    \n",
    "    # 2. Exclude vertices that have already been visited.\n",
    "    new_neighbors = new_neighbors.join(visited, on=\"vertex\", how=\"left_anti\").distinct()\n",
    "    \n",
    "    # 3. If no new vertices are found, exit the loop.\n",
    "    if new_neighbors.count() == 0:\n",
    "        break\n",
    "    \n",
    "    # 4. Add the new neighbors to the visited set.\n",
    "    visited = visited.union(new_neighbors).distinct()\n",
    "    \n",
    "    # 5. Update the frontier to be the new neighbors.\n",
    "    frontier = new_neighbors\n",
    "\n",
    "# The 'visited' DataFrame now contains all vertices reachable from the source,\n",
    "# along with the minimum number of steps (distance) from the source.\n",
    "visited.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81313c63-0bd1-4bbb-8838-6c21746de287",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cc2f4-919b-4c97-8725-9ce254187118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create or get your Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = edge_df.limit(10000) # Change as needed\n",
    "\n",
    "# Set the reset (teleportation) probability and the number of iterations\n",
    "alpha = 0.15\n",
    "maxIter = 10\n",
    "\n",
    "# Choose the personalized seed: take the first value from col_b\n",
    "seed = df.select(\"col_b\").first()[0]\n",
    "\n",
    "# 1. Create the vertices DataFrame: union of unique IDs from col_b and col_d.\n",
    "vertices = (\n",
    "    df.select(F.col(\"col_b\").alias(\"id\"))\n",
    "      .union(df.select(F.col(\"col_d\").alias(\"id\")))\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# 2. Create the edges DataFrame: define edge from col_b to col_d.\n",
    "edges = df.select(F.col(\"col_b\").alias(\"src\"), F.col(\"col_d\").alias(\"dst\"))\n",
    "\n",
    "# 3. Compute out-degrees: count of outgoing edges for each source vertex.\n",
    "out_degrees = edges.groupBy(\"src\").agg(F.count(\"*\").alias(\"out_degree\"))\n",
    "\n",
    "# 4. Initialize each vertex with a PageRank value:\n",
    "#    The seed gets 1.0 and all others start with 0.0.\n",
    "vertices_rank = vertices.withColumn(\n",
    "    \"rank\", F.when(F.col(\"id\") == seed, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# 5. Iteratively update the PageRank values.\n",
    "for i in range(maxIter):\n",
    "    # 5a. For each edge, compute the contribution from its source.\n",
    "    #     Join the edges with the current vertex ranks and the out-degrees.\n",
    "    contribs = (\n",
    "        edges.join(vertices_rank, edges.src == vertices_rank.id)\n",
    "             .join(out_degrees, edges.src == out_degrees.src)\n",
    "             .select(\n",
    "                 edges.dst.alias(\"id\"),\n",
    "                 (vertices_rank.rank / out_degrees.out_degree).alias(\"contrib\")\n",
    "             )\n",
    "    )\n",
    "    \n",
    "    # 5b. Sum the contributions arriving at each vertex.\n",
    "    contribs_sum = contribs.groupBy(\"id\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "    \n",
    "    # 5c. Compute the total rank from dangling nodes (vertices with no outgoing edges).\n",
    "    dangling = (\n",
    "        vertices_rank.join(out_degrees, vertices_rank.id == out_degrees.src, \"left\")\n",
    "                     .withColumn(\"out_degree\", F.coalesce(F.col(\"out_degree\"), F.lit(0)))\n",
    "                     .where(F.col(\"out_degree\") == 0)\n",
    "    )\n",
    "    dangling_sum = dangling.agg(F.sum(\"rank\").alias(\"dangling_sum\")).collect()[0][\"dangling_sum\"]\n",
    "    if dangling_sum is None:\n",
    "        dangling_sum = 0.0\n",
    "    \n",
    "    # 5d. Update each vertex's rank:\n",
    "    #     - If the vertex is the seed, it gets the reset term (alpha)\n",
    "    #       plus (1 - alpha) times (its incoming contributions plus dangling rank).\n",
    "    #     - Otherwise, it just gets (1 - alpha) times its incoming contributions.\n",
    "    vertices_rank = (\n",
    "        vertices.join(contribs_sum, on=\"id\", how=\"left\")\n",
    "                .na.fill({\"sum_contrib\": 0.0})\n",
    "                .withColumn(\"rank\", \n",
    "                    F.when(F.col(\"id\") == seed,\n",
    "                           alpha + (1 - alpha) * (F.col(\"sum_contrib\") + dangling_sum)\n",
    "                    ).otherwise(\n",
    "                           (1 - alpha) * F.col(\"sum_contrib\")\n",
    "                    )\n",
    "                )\n",
    "    )\n",
    "\n",
    "# 6. Display the final personalized PageRank values.\n",
    "vertices_rank.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
