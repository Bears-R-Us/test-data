{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934fd04-4acd-4d1c-be90-b24a612cb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, BooleanType, DateType, TimestampType, ArrayType\n",
    ")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"spark://masternode\") \\\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.executor.cores\",12) \\\n",
    "    .config(\"spark.executor.instances\",4) \\\n",
    "    .config(\"spark.executor.memory\",\"16G\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\",\"160mb\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\",\"32mb\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\",\"2gb\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\",200) \\\n",
    "    .config(\"spark.task.cpus\",1) \\\n",
    "    .config(\"spark.sql.legacy.charVarcharAsString\",True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c6c4b-c699-4958-be69-d2b3d3804658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_synthetic_distribution(params, plot=True):\n",
    "\n",
    "    slope = params.get('slope', -2)\n",
    "    min_degree = params.get('min_degree', 1)\n",
    "    max_degree = params.get('max_degree', 200_000)\n",
    "    max_prob = params.get('max_prob', 0.5)\n",
    "\n",
    "    # Create an array of degrees from min_degree to max_degree as floats\n",
    "    degrees = np.arange(min_degree, max_degree + 1, dtype=float)\n",
    "\n",
    "    # Calculate the scaling factor A to ensure the maximum probability at min_degree\n",
    "    A = max_prob / (min_degree ** slope)\n",
    "\n",
    "    # Compute the power-law decay values\n",
    "    y_values = A * degrees ** slope\n",
    "\n",
    "    # Convert degrees to integers for dictionary keys\n",
    "    degrees_int = degrees.astype(int)\n",
    "\n",
    "    # Create a dictionary mapping degrees to decay values\n",
    "    decay_dict = dict(zip(degrees_int, y_values))\n",
    "\n",
    "    return decay_dict\n",
    "\n",
    "params = {\n",
    "    'slope': -2,\n",
    "    'intercept': 0.8,\n",
    "    'r_squared': 0.98,\n",
    "    'max_degree': 200_000,\n",
    "    'min_degree': 1,\n",
    "    'max_prob': 0.5,\n",
    "    'degree_range': list(np.arange(1, 200_000))\n",
    "}\n",
    "\n",
    "target_distribution = create_synthetic_distribution(params, 200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a61f6e-1da4-421c-9a1a-b25184b76188",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = spark.sparkContext.defaultParallelism # number of cores available\n",
    "num_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd09b9-16fb-4c61-8cb5-084040f0e70e",
   "metadata": {},
   "source": [
    "# Create Graph Based on Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bcb81-94af-4fcb-b5f4-14d9c74bbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import random\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"NetworkFlowGraph\").getOrCreate()\n",
    "\n",
    "def random_node():\n",
    "    # return '.'.join(map(str, np.random.randint(0, 256, size=4).tolist()))\n",
    "    return int(np.random.randint(1_000_000, 10_000_000_000))\n",
    "\n",
    "def random_feature():\n",
    "    return int(np.random.randint(1, 70000))  # cast to native int\n",
    "\n",
    "def random_col_e():\n",
    "    return str(np.random.choice(['col_e_A', 'col_e_B']))  # cast to native str\n",
    "\n",
    "# num_graphs = spark.sparkContext.defaultParallelism # number of cores available\n",
    "num_nodes_per_graph = 350_000\n",
    "\n",
    "def configuration_model_with_distribution(n, degree_distribution,seed):\n",
    "    \"\"\"\n",
    "    Generate a graph with a specific degree distribution\n",
    "    \"\"\"\n",
    "\n",
    "    degrees = []\n",
    "    remaining_nodes = n\n",
    "\n",
    "    for degree, prob in sorted(degree_distribution.items()):\n",
    "        if remaining_nodes <= 0:\n",
    "            break\n",
    "        count = min(int(n * prob + 0.5), remaining_nodes)\n",
    "        if count > 0:\n",
    "            degrees.extend([int(degree)] * count)\n",
    "            remaining_nodes -= count\n",
    "\n",
    "    if remaining_nodes > 0:\n",
    "        min_degree = min(degree_distribution.keys())\n",
    "        degrees.extend([min_degree] * remaining_nodes)\n",
    "\n",
    "    if len(degrees) < 2:\n",
    "        degrees = [1, 1]\n",
    "\n",
    "    if sum(degrees) % 2 != 0:\n",
    "        degrees[0] += 1\n",
    "\n",
    "    try:\n",
    "        g = nx.configuration_model(degrees, seed=seed)\n",
    "        g = nx.Graph(g)\n",
    "\n",
    "        if g.number_of_edges() == 0:\n",
    "            raise nx.NetworkXError(\"Generated graph has no edges\")\n",
    "\n",
    "        return g\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating graph: {e}\")\n",
    "        return nx.barabasi_ablert_graph(n, 2)\n",
    "    \n",
    "def generate_custom_graph(partition_id, num_nodes, degree_distribution, seed):\n",
    "    # np.random.seed(seed + partition_id)\n",
    "    # random.seed(seed + partition_id)\n",
    "\n",
    "    g = configuration_model_with_distribution(num_nodes, degree_distribution, seed + partition_id)\n",
    "\n",
    "    node_map = {node: random_node() for node in g.nodes()}\n",
    "    edges = [(node_map[edge[0]], node_map[edge[1]],\n",
    "              random_feature(), random_feature(), random_col_e())\n",
    "             for edge in g.edges()]\n",
    "    \n",
    "    return edges\n",
    "\n",
    "target_distribution_bc = spark.sparkContext.broadcast(target_distribution)\n",
    "\n",
    "directory_path = \"/path/to/synthetic_dataset\"\n",
    "seed = 1\n",
    "remaining_iterations = 10\n",
    "\n",
    "# while get_directory_size(directory_path) < MAX_SIZE_BYTES:\n",
    "start = time.time()\n",
    "while remaining_iterations > 0:\n",
    "    print(remaining_iterations)\n",
    "    seed_bc = spark.sparkContext.broadcast(seed)\n",
    "    generate_custom_graph_udf = udf(\n",
    "        lambda partition_id: generate_custom_graph(\n",
    "            partition_id, num_nodes_per_graph, target_distribution_bc.value, seed_bc.value\n",
    "        ),\n",
    "        ArrayType(StructType([\n",
    "            StructField(\"col_a\", LongType(), False),\n",
    "            StructField(\"col_b\", LongType(), False),\n",
    "            StructField(\"col_c\", IntegerType(), False),\n",
    "            StructField(\"col_d\", IntegerType(), False),\n",
    "            StructField(\"col_e\", StringType(), False),\n",
    "        ]))\n",
    "    )\n",
    "\n",
    "    edge_df = (\n",
    "        spark.range(num_graphs)\n",
    "             .withColumn(\"edges\", generate_custom_graph_udf(\"id\"))\n",
    "             .select(explode(\"edges\").alias(\"edge\"))\n",
    "             .select(\"edge.col_a\", \"edge.col_b\", \"edge.col_c\", \"edge.col_d\", \"edge.col_e\")\n",
    "             .distinct()  # if needed\n",
    "    )\n",
    "\n",
    "    edge_df.write.mode(\"append\").parquet(directory_path)\n",
    "\n",
    "    seed += 1\n",
    "    remaining_iterations -= 1\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TTR 100 Iterations: {round(end-start, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467b604-4222-41e1-9279-12cc7d91481f",
   "metadata": {},
   "source": [
    "# Alternative Option for Scaling: Double Size of Dataset and Add Random Noise (maintains distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5b98b-e6a1-4dcc-b9f8-78c34a02a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, floor, rand, lit\n",
    "\n",
    "# Define the noise range\n",
    "NOISE_MIN = -5\n",
    "NOISE_MAX = 5\n",
    "\n",
    "long_cols = [\"col_a\", \"col_b\"]\n",
    "integer_cols = [\"col_c\", \"col_d\"]\n",
    "string_cols = [\"col_e\"]\n",
    "\n",
    "# Original DataFrame\n",
    "df_original = test\n",
    "\n",
    "# Create the noisy copy\n",
    "df_augmented = df_original.select(\n",
    "    *[\n",
    "        # Add integer noise to numeric columns\n",
    "        (col(c) + floor(rand() * (NOISE_MAX - NOISE_MIN + 1)) + NOISE_MIN).cast(\"long\").alias(c)\n",
    "        for c in long_cols\n",
    "    ] + [\n",
    "        # Add integer noise to numeric columns\n",
    "        (col(c) + floor(rand() * (NOISE_MAX - NOISE_MIN + 1)) + NOISE_MIN).cast(\"integer\").alias(c)\n",
    "        for c in integer_cols\n",
    "    ] + [\n",
    "        # Preserve string columns\n",
    "        col(c) for c in string_cols\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Optionally add source labels\n",
    "df_original = df_original.withColumn(\"source\", lit(\"original\"))\n",
    "df_augmented = df_augmented.withColumn(\"source\", lit(\"noisy\"))\n",
    "\n",
    "# Combine both\n",
    "df_doubled = df_original.unionByName(df_augmented)\n",
    "\n",
    "df_doubled.repartition(2000).write.parquet('/path/to/synthetic_dataset_doubled', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06576-9d36-4ca0-a047-1fd8b65b5b71",
   "metadata": {},
   "source": [
    "# Get Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abe78e-109e-4704-bdcb-43590b4fb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import random\n",
    "\n",
    "edge_df = df_doubled\n",
    "\n",
    "def print_graph_stats(edge_df, display=False):\n",
    "\n",
    "    unique_nodes = edge_df.select(\"col_a\").union(edge_df.select(\"col_b\")).distinct()\n",
    "\n",
    "    total_edges = edge_df.count()\n",
    "\n",
    "    src_degrees = edge_df.groupBy(\"col_a\").agg({\"col_a\": \"count\"}) \\\n",
    "        .withColumnRenamed(\"count(col_a)\", \"in_degree\") \\\n",
    "        .withColumnRenamed(\"col_a\", \"merge_col\")\n",
    "\n",
    "    dst_degrees = edge_df.groupBy(\"col_b\").agg({\"col_b\": \"count\"}) \\\n",
    "        .withColumnRenamed(\"count(col_b)\", \"out_degree\") \\\n",
    "        .withColumnRenamed(\"col_b\", \"merge_col\")\n",
    "\n",
    "    total_degrees = src_degrees.join(dst_degrees, \"merge_col\", \"outer\") \\\n",
    "        .fillna(0) \\\n",
    "            .selectExpr(\n",
    "                \"merge_col\", \n",
    "                \"cast(out_degree as int) + cast(in_degree as int) as total_degree\"\n",
    "            )\n",
    "        \n",
    "    avg_degree = total_degrees.agg({\"total_degree\": \"avg\"}).collect()[0][0]\n",
    "    max_degree = total_degrees.agg({\"total_degree\": \"max\"}).collect()[0][0]\n",
    "\n",
    "    if display:\n",
    "        print(\"\\nGraph Statistics:\")\n",
    "        print(f\"Number of unique Nodes: {unique_nodes.count()}\")\n",
    "        print(f\"Number of edges: {total_edges}\")\n",
    "        print(f\"Average degree: {avg_degree:.2f}\")\n",
    "        print(f\"Maximum degree: {max_degree}\")\n",
    "\n",
    "    return total_degrees\n",
    "\n",
    "def check_degree_distribution(edge_df, plot=False):\n",
    "    total_degrees = print_graph_stats(edge_df)\n",
    "\n",
    "    degree_dist = total_degrees.groupBy(\"total_degree\").count() \\\n",
    "        .withColumnRenamed(\"count\", \"num_vertices\")\n",
    "\n",
    "    total_nodes = total_degrees.count()\n",
    "\n",
    "    # Collect results to driver for plotting\n",
    "    dist_rows = degree_dist.collect()\n",
    "    data = {\n",
    "        \"total_degree\": [row[\"total_degree\"] for row in dist_rows],\n",
    "        \"num_vertices\": [row[\"num_vertices\"] for row in dist_rows]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"empirical\"] = df[\"num_vertices\"] / total_nodes\n",
    "    df[\"target\"] = df[\"total_degree\"].map(target_distribution).fillna(1e-12)  # avoid log(0)\n",
    "    \n",
    "    # Sort by total_degree\n",
    "    df = df.sort_values(by=\"total_degree\")\n",
    "\n",
    "    print(\"\\nResulting degree distribution:\")\n",
    "    for row in df.itertuples():\n",
    "        print(f\"Degree {row.total_degree}: {row.empirical:.4f} (Target: {row.target:.4f})\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['total_degree'], df['empirical'], marker='o', label='Empirical Distribution')\n",
    "        plt.plot(df['total_degree'], df['target'], marker='x', linestyle='--', label='Target Distribution')\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Percentage of Nodes (log scale)')\n",
    "        plt.yscale('log')\n",
    "        plt.title('Degree Distribution: Empirical vs Target (Log Scale)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print_graph_stats(edge_df, display=True)\n",
    "check_degree_distribution(edge_df, True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
